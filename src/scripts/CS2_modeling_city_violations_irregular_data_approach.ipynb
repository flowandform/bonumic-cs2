{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideOutput": false
   },
   "source": [
    "# VIOLATIONS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_data = pd.read_csv('/home/deploy/notebooks/city_violations_data/input/SF_download/violations/Notices_of_violation_SF.csv',\n",
    "                sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrangling some columns\n",
    "\n",
    "# converting column to date\n",
    "viol_data['date'] = pd.to_datetime(viol_data['Date Filed'], format='%m/%d/%Y')\n",
    "\n",
    "# creating column which will be a basis for prediction - block_lot (parcel)\n",
    "viol_data['block_lot'] = viol_data['Block'].astype(str) + '_' + viol_data['Lot'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_data.drop(['Date Filed','Complaint Number', 'Item Sequence Number', 'Street Suffix', 'Unit','Receiving Division','Assigned Division',\n",
    "        'Zipcode', 'NOV Item Description',  ], axis =1 , inplace = True   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in viol_data.columns:\n",
    "    print('{}: {}'.format(col, len(viol_data[col].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in viol_data.columns:\n",
    "    print(col, viol_data[col].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### group level feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_df = viol_data.copy()\n",
    "viol_df = viol_df.sort_values(by='date')\n",
    "\n",
    "#################################### TIER I DERIVATED VARIABLES ####################################\n",
    "\n",
    "# A) calculating days since last violations for 3 most recent violations ocurred \n",
    "auxiliary_viol_df = viol_df[['block_lot', 'date']].drop_duplicates()\n",
    "auxiliary_viol_df['date_shift_one'] = auxiliary_viol_df.groupby(['block_lot'])['date'].shift(+1)\n",
    "auxiliary_viol_df['date_shift_two'] = auxiliary_viol_df.groupby(['block_lot'])['date'].shift(+2)\n",
    "auxiliary_viol_df['date_shift_three'] = auxiliary_viol_df.groupby(['block_lot'])['date'].shift(+3)\n",
    "auxiliary_viol_df['date_shift_four'] = auxiliary_viol_df.groupby(['block_lot'])['date'].shift(+4)\n",
    "auxiliary_viol_df['date_shift_five'] = auxiliary_viol_df.groupby(['block_lot'])['date'].shift(+5)\n",
    "\n",
    "viol_df = viol_df.merge(auxiliary_viol_df, on = ['block_lot', 'date'], how = 'left')\n",
    "\n",
    "# calculating timedelta\n",
    "viol_df['days_since_last_viol'] = viol_df['date'] - viol_df['date_shift_one']\n",
    "viol_df['days_since_last_viol_shift'] = viol_df['date'] - viol_df['date_shift_two']\n",
    "viol_df['days_since_last_viol_shift_two'] = viol_df['date'] - viol_df['date_shift_three']\n",
    "viol_df['days_since_last_viol_shift_three'] = viol_df['date'] - viol_df['date_shift_four']\n",
    "viol_df['days_since_last_viol_shift_four'] = viol_df['date'] - viol_df['date_shift_five']\n",
    "\n",
    "# converting to integer\n",
    "viol_df['days_since_last_viol'] = viol_df['days_since_last_viol'].dt.days\n",
    "viol_df['days_since_last_viol_shift'] =viol_df['days_since_last_viol_shift'].dt.days\n",
    "viol_df['days_since_last_viol_shift_two'] = viol_df['days_since_last_viol_shift_two'].dt.days\n",
    "viol_df['days_since_last_viol_shift_three'] = viol_df['days_since_last_viol_shift_three'].dt.days\n",
    "viol_df['days_since_last_viol_shift_four'] = viol_df['days_since_last_viol_shift_four'].dt.days\n",
    "\n",
    "# replacing null values with zero\n",
    "viol_df['days_since_last_viol'] = viol_df['days_since_last_viol'].replace(np.nan, 0)\n",
    "viol_df['days_since_last_viol_shift'] = viol_df['days_since_last_viol_shift'].replace(np.nan, 0)\n",
    "viol_df['days_since_last_viol_shift_two'] = viol_df['days_since_last_viol_shift_two'].replace(np.nan, 0)\n",
    "viol_df['days_since_last_viol_shift_three'] = viol_df['days_since_last_viol_shift_three'].replace(np.nan, 0)\n",
    "viol_df['days_since_last_viol_shift_four'] = viol_df['days_since_last_viol_shift_four'].replace(np.nan, 0)\n",
    "\n",
    "\n",
    "# B) creating number of viol for 3 most recent violations ocurred  \n",
    "auxiliary_viol_df=  viol_df.groupby(['block_lot', 'date']).size().reset_index(name='N_violations_current_insp')\n",
    "auxiliary_viol_df['N_violations_last_insp'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_current_insp'].shift(+1)\n",
    "auxiliary_viol_df['N_violations_last_insp_shift'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_current_insp'].shift(+2)\n",
    "auxiliary_viol_df['N_violations_last_insp_shift_two'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_current_insp'].shift(+3)\n",
    "auxiliary_viol_df['N_violations_last_insp_shift_three'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_current_insp'].shift(+4)\n",
    "auxiliary_viol_df['N_violations_last_insp_shift_four'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_current_insp'].shift(+5)\n",
    "\n",
    "viol_df = viol_df.merge(auxiliary_viol_df, on = ['block_lot', 'date'], how = 'left')\n",
    "\n",
    "# replacing null values with zero\n",
    "viol_df['N_violations_current_insp'] = viol_df['N_violations_current_insp'].replace(np.nan, 0)\n",
    "viol_df['N_violations_last_insp'] = viol_df['N_violations_last_insp'].replace(np.nan, 0)\n",
    "viol_df['N_violations_last_insp_shift'] = viol_df['N_violations_last_insp_shift'].replace(np.nan, 0)\n",
    "viol_df['N_violations_last_insp_shift_two'] = viol_df['N_violations_last_insp_shift_two'].replace(np.nan, 0)\n",
    "viol_df['N_violations_last_insp_shift_three'] = viol_df['N_violations_last_insp_shift_three'].replace(np.nan, 0)\n",
    "viol_df['N_violations_last_insp_shift_four'] = viol_df['N_violations_last_insp_shift_four'].replace(np.nan, 0)\n",
    "\n",
    "\n",
    "# C) creating cummulative violations till date for 3 most recent violations ocurred \n",
    "auxiliary_viol_df['violations_cumsum_till_date'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_current_insp'].cumsum()\n",
    "auxiliary_viol_df['violations_cumsum_till_date_shift'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_last_insp'].cumsum()\n",
    "auxiliary_viol_df['violations_cumsum_till_date_shift_two'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_last_insp_shift'].cumsum()\n",
    "auxiliary_viol_df['violations_cumsum_till_date_shift_three'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_last_insp_shift_two'].cumsum()\n",
    "auxiliary_viol_df['violations_cumsum_till_date_shift_four'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_last_insp_shift_three'].cumsum()\n",
    "\n",
    "# dropping duplicate columns\n",
    "auxiliary_viol_df = auxiliary_viol_df.drop([\n",
    "    'N_violations_current_insp',\n",
    "    'N_violations_last_insp',\n",
    "    'N_violations_last_insp_shift',\n",
    "    'N_violations_last_insp_shift_two',\n",
    "    'N_violations_last_insp_shift_three',\n",
    "    'N_violations_last_insp_shift_four'],\n",
    "    axis =1)\n",
    "viol_df = viol_df.merge(auxiliary_viol_df, on = ['block_lot', 'date'], how = 'left')\n",
    "\n",
    "# replacing null values with zero\n",
    "viol_df['violations_cumsum_till_date'] = viol_df['violations_cumsum_till_date'].replace(np.nan, 0)\n",
    "viol_df['violations_cumsum_till_date_shift'] = viol_df['violations_cumsum_till_date_shift'].replace(np.nan, 0)\n",
    "viol_df['violations_cumsum_till_date_shift_two'] = viol_df['violations_cumsum_till_date_shift_two'].replace(np.nan, 0)\n",
    "viol_df['violations_cumsum_till_date_shift_three'] = viol_df['violations_cumsum_till_date_shift_three'].replace(np.nan, 0)\n",
    "viol_df['violations_cumsum_till_date_shift_four'] = viol_df['violations_cumsum_till_date_shift_four'].replace(np.nan, 0)\n",
    "\n",
    "\n",
    "\n",
    "#################################### TIER 2 DERIVATED VARIABLES ####################################\n",
    "# helper lists\n",
    "days_since_cols = ['days_since_last_viol','days_since_last_viol_shift','days_since_last_viol_shift_two',\n",
    "                      'days_since_last_viol_shift_three','days_since_last_viol_shift_four']\n",
    "\n",
    "N_violations_recent_insp_cols = ['N_violations_current_insp','N_violations_last_insp','N_violations_last_insp_shift',\n",
    "                                 'N_violations_last_insp_shift_two','N_violations_last_insp_shift_three',\n",
    "                                 'N_violations_last_insp_shift_four']\n",
    "\n",
    "# creating overal averages and stdevs\n",
    "viol_df['avg_days_between_previous_violations'] = viol_df[days_since_cols].mean(axis = 1)\n",
    "viol_df['avg_N_violations_recent_inspections'] = viol_df[N_violations_recent_insp_cols].mean(axis = 1)\n",
    "viol_df['StDev_days_between_previous_violations'] = viol_df[days_since_cols].std(axis = 1)\n",
    "viol_df['StDev_days_between_previous_violations'] = viol_df[days_since_cols].std(axis = 1)\n",
    "viol_df['ratio_avg_days_avg_N_violations'] = viol_df['avg_days_between_previous_violations']/viol_df['avg_N_violations_recent_inspections']\n",
    "\n",
    "# creating individual vs rest ratios\n",
    "for col in days_since_cols:\n",
    "    viol_df[\"{}_ratio_other\".format(col)] = viol_df[col] / viol_df[[x for x in days_since_cols if x!= col]].mean(axis=1)\n",
    "    viol_df[\"{}_ratio_other\".format(col)]= viol_df[\"{}_ratio_other\".format(col)].replace(np.nan, 0)\n",
    "\n",
    "for col in N_violations_recent_insp_cols:\n",
    "    viol_df[\"{}_ratio_other\".format(col)] = viol_df[col] / viol_df[[x for x in N_violations_recent_insp_cols if x!= col]].mean(axis=1)\n",
    "    viol_df[\"{}_ratio_other\".format(col)] = viol_df[\"{}_ratio_other\".format(col)] .replace(np.nan, 0)\n",
    "\n",
    "# adding columnwise subtractions\n",
    "auxiliary_viol_df = viol_df[days_since_cols]\n",
    "auxiliary_viol_df = auxiliary_viol_df.shift(-1, axis = 1) - auxiliary_viol_df \n",
    "auxiliary_viol_df = auxiliary_viol_df.iloc[:, :-1]\n",
    "auxiliary_viol_df.columns = [\"{}_subtracted\".format(x) for x in auxiliary_viol_df.columns]\n",
    "auxiliary_viol_df['AVG_days_since_columnwise_diff'] = auxiliary_viol_df.mean(axis = 1)\n",
    "auxiliary_viol_df['StDev_days_since_columnwise_diff'] = auxiliary_viol_df.std(axis = 1)\n",
    "viol_df = pd.concat([viol_df, auxiliary_viol_df], axis =1)\n",
    "\n",
    "auxiliary_viol_df = viol_df[N_violations_recent_insp_cols]\n",
    "auxiliary_viol_df = auxiliary_viol_df.iloc[:, :-1]\n",
    "auxiliary_viol_df = auxiliary_viol_df.shift(-1, axis = 1) - auxiliary_viol_df \n",
    "auxiliary_viol_df.columns = [\"{}_subtracted\".format(x) for x in auxiliary_viol_df.columns]\n",
    "auxiliary_viol_df['AVG_N_violations_columnwise_diff'] = auxiliary_viol_df.mean(axis = 1)\n",
    "auxiliary_viol_df['StDev_N_violations_columnwise_diff'] = auxiliary_viol_df.std(axis = 1)\n",
    "viol_df = pd.concat([viol_df, auxiliary_viol_df], axis =1)\n",
    "\n",
    "auxiliary_viol_df['AVG_days_since_N_violations_columnwise_diff'] =viol_df['AVG_days_since_columnwise_diff']\\\n",
    "                                                            /viol_df['AVG_N_violations_columnwise_diff']\n",
    "\n",
    "# adding columnwise ratios\n",
    "auxiliary_viol_df = viol_df[days_since_cols]\n",
    "auxiliary_viol_df = auxiliary_viol_df.shift(-1, axis = 1)/auxiliary_viol_df \n",
    "auxiliary_viol_df = auxiliary_viol_df.iloc[:, :-1]\n",
    "auxiliary_viol_df.columns = [\"{}_ratio_previous\".format(x) for x in auxiliary_viol_df.columns]\n",
    "viol_df[\"{}_ratio_other\".format(col)] = viol_df[\"{}_ratio_other\".format(col)] .replace(np.nan, 0)\n",
    "\n",
    "auxiliary_viol_df['AVG_days_since_columnwise_ratio'] = auxiliary_viol_df.mean(axis = 1)\n",
    "auxiliary_viol_df['StDev_days_since_columnwise_ratio'] = auxiliary_viol_df.std(axis = 1)\n",
    "viol_df = pd.concat([viol_df, auxiliary_viol_df], axis =1)\n",
    "\n",
    "auxiliary_viol_df = viol_df[N_violations_recent_insp_cols]\n",
    "auxiliary_viol_df = auxiliary_viol_df.shift(-1, axis = 1)/auxiliary_viol_df \n",
    "auxiliary_viol_df = auxiliary_viol_df.iloc[:, :-1]\n",
    "auxiliary_viol_df.columns = [\"{}_ratio_previous\".format(x) for x in auxiliary_viol_df.columns]\n",
    "auxiliary_viol_df['AVG_N_violations_columnwise_ratio'] = auxiliary_viol_df.mean(axis = 1)\n",
    "auxiliary_viol_df['StDev_N_violations_columnwise_ratio'] = auxiliary_viol_df.std(axis = 1)\n",
    "viol_df = pd.concat([viol_df, auxiliary_viol_df], axis =1)\n",
    "\n",
    "cum_sum_cols = ['violations_cumsum_till_date', 'violations_cumsum_till_date_shift','violations_cumsum_till_date_shift_two',\n",
    "                'violations_cumsum_till_date_shift_three','violations_cumsum_till_date_shift_four']\n",
    "auxiliary_viol_df = viol_df[cum_sum_cols]\n",
    "auxiliary_viol_df = auxiliary_viol_df/auxiliary_viol_df.shift(-1, axis = 1) \n",
    "auxiliary_viol_df = auxiliary_viol_df.iloc[:, :-1]\n",
    "auxiliary_viol_df.columns = [\"{}_ratio_previous\".format(x) for x in auxiliary_viol_df.columns]\n",
    "auxiliary_viol_df['AVG_cumsum_columnwise_ratio'] = auxiliary_viol_df.mean(axis = 1)\n",
    "auxiliary_viol_df['StDev_cumsum_columnwise_ratio'] = auxiliary_viol_df.std(axis = 1)\n",
    "viol_df = pd.concat([viol_df, auxiliary_viol_df], axis =1)\n",
    "\n",
    "# adding inter group columnwise ratios\n",
    "viol_df['ratio_days_since_N_violations_last_insp'] = viol_df['days_since_last_viol']/viol_df['N_violations_last_insp']\n",
    "viol_df['ratio_days_since_N_violations_last_insp_shift'] = viol_df['days_since_last_viol_shift']/viol_df['N_violations_last_insp_shift']\n",
    "viol_df['ratio_days_since_N_violations_last_insp_shift_two'] = viol_df['days_since_last_viol_shift_two']/viol_df['N_violations_last_insp_shift_two']\n",
    "viol_df['ratio_days_since_N_violations_last_insp_shift_three'] = viol_df['days_since_last_viol_shift_three']/viol_df['N_violations_last_insp_shift_three']\n",
    "viol_df['ratio_days_since_N_violations_last_insp_shift_four'] = viol_df['days_since_last_viol_shift_four']/viol_df['N_violations_last_insp_shift_four']\n",
    "\n",
    "\n",
    "viol_df['AVG_columnwise_ratio_days_since_N_violations'] = viol_df[['ratio_days_since_N_violations_last_insp',\n",
    "                                                          'ratio_days_since_N_violations_last_insp_shift_two',\n",
    "                                                          'ratio_days_since_N_violations_last_insp_shift_three',\n",
    "                                                          'ratio_days_since_N_violations_last_insp_shift_four']].mean(axis = 1)\n",
    "\n",
    "viol_df['StDev_columnwise_ratio_days_since_N_violations'] = viol_df[['ratio_days_since_N_violations_last_insp',\n",
    "                                                          'ratio_days_since_N_violations_last_insp_shift_two',\n",
    "                                                          'ratio_days_since_N_violations_last_insp_shift_three',\n",
    "                                                          'ratio_days_since_N_violations_last_insp_shift_four']].std(axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### individual category level feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################################### TIER I DERIVATED VARIABLES ####################################\n",
    "\n",
    "# A) calculating days since last violations for 3 most recent violations ocurred \n",
    "for i in viol_df['NOV Category Description'].unique():\n",
    "    print(i)\n",
    "    \n",
    "    auxiliary_viol_df = viol_df[['block_lot', 'date','NOV Category Description']]\n",
    "    auxiliary_viol_df['{}_date'.format(i)]= np.where(auxiliary_viol_df['NOV Category Description'] == i, 1, np.nan)\n",
    "    \n",
    "    auxiliary_viol_df = auxiliary_viol_df.dropna(subset= ['{}_date'.format(i)])\n",
    "    auxiliary_viol_df['{}_date'.format(i)] = auxiliary_viol_df['date']\n",
    "    auxiliary_viol_df = auxiliary_viol_df.drop_duplicates(subset = ['block_lot', 'date'])\n",
    "    auxiliary_viol_df = auxiliary_viol_df.drop(['NOV Category Description'], axis =1)\n",
    "\n",
    "    auxiliary_viol_df['{}_date_shift'.format(i)] = auxiliary_viol_df.groupby(['block_lot'])['{}_date'.format(i)].shift(+1)\n",
    "    auxiliary_viol_df['{}_date_shift_two'.format(i)] = auxiliary_viol_df.groupby(['block_lot'])['{}_date'.format(i)].shift(+2)\n",
    "    auxiliary_viol_df['{}_date_shift_three'.format(i)] = auxiliary_viol_df.groupby(['block_lot'])['{}_date'.format(i)].shift(+3)\n",
    "    auxiliary_viol_df['{}_date_shift_four'.format(i)] = auxiliary_viol_df.groupby(['block_lot'])['{}_date'.format(i)].shift(+4)\n",
    "\n",
    "    viol_df = viol_df.merge(auxiliary_viol_df, on = ['block_lot', 'date'], how = 'left')\n",
    "    viol_df['{}_date_ffill'.format(i)] = viol_df.groupby(['block_lot'])['{}_date'.format(i)].ffill()\n",
    "    viol_df['{}_date_ffill_shift'.format(i)] = viol_df.groupby(['block_lot'])['{}_date_shift'.format(i)].ffill()\n",
    "    viol_df['{}_ffill_shift_two'.format(i)] = viol_df.groupby(['block_lot'])['{}_date_shift_two'.format(i)].ffill()\n",
    "    viol_df['{}_ffill_shift_three'.format(i)] = viol_df.groupby(['block_lot'])['{}_date_shift_three'.format(i)].ffill()\n",
    "    viol_df['{}_ffill_shift_four'.format(i)] = viol_df.groupby(['block_lot'])['{}_date_shift_four'.format(i)].ffill()\n",
    "\n",
    "    viol_df['days_since_{}'.format(i)] = viol_df['date'] - viol_df['{}_date_ffill'.format(i)]\n",
    "    viol_df['days_since_{}_shift'.format(i)] = viol_df['date'] - viol_df['{}_date_ffill_shift'.format(i)]\n",
    "    viol_df['days_since_{}_shift_two'.format(i)] = viol_df['date'] - viol_df['{}_ffill_shift_two'.format(i)]\n",
    "    viol_df['days_since_{}_shift_three'.format(i)] = viol_df['date'] - viol_df['{}_ffill_shift_three'.format(i)]\n",
    "    viol_df['days_since_{}_shift_four'.format(i)] = viol_df['date'] - viol_df['{}_ffill_shift_four'.format(i)]\n",
    "    \n",
    "    viol_df['days_since_{}'.format(i)] =  viol_df['days_since_{}'.format(i)].dt.days\n",
    "    viol_df['days_since_{}_shift'.format(i)] =  viol_df['days_since_{}_shift'.format(i)].dt.days\n",
    "    viol_df['days_since_{}_shift_two'.format(i)] =  viol_df['days_since_{}_shift_two'.format(i)].dt.days\n",
    "    viol_df['days_since_{}_shift_three'.format(i)] =  viol_df['days_since_{}_shift_three'.format(i)].dt.days\n",
    "    viol_df['days_since_{}_shift_four'.format(i)] = viol_df['days_since_{}_shift_four'.format(i)].dt.days   \n",
    "    \n",
    "\n",
    "    viol_df['days_since_{}'.format(i)] =  viol_df['days_since_{}'.format(i)].replace(np.nan, 0)\n",
    "    viol_df['days_since_{}_shift'.format(i)] =  viol_df['days_since_{}_shift'.format(i)].replace(np.nan, 0)\n",
    "    viol_df['days_since_{}_shift_two'.format(i)] =  viol_df['days_since_{}_shift_two'.format(i)].replace(np.nan, 0)\n",
    "    viol_df['days_since_{}_shift_three'.format(i)] =  viol_df['days_since_{}_shift_three'.format(i)].replace(np.nan, 0)\n",
    "    viol_df['days_since_{}_shift_four'.format(i)] = viol_df['days_since_{}_shift_four'.format(i)].replace(np.nan, 0)\n",
    "\n",
    "    viol_df = viol_df.drop([\n",
    "        '{}_date'.format(i),\n",
    "        '{}_date_shift'.format(i),\n",
    "        '{}_date_shift_two'.format(i),\n",
    "        '{}_date_shift_three'.format(i),\n",
    "        '{}_date_shift_four'.format(i),\n",
    "        '{}_date_ffill'.format(i),\n",
    "        '{}_date_ffill_shift'.format(i),\n",
    "        '{}_ffill_shift_two'.format(i),\n",
    "        '{}_ffill_shift_three'.format(i),\n",
    "        '{}_ffill_shift_four'.format(i)],\n",
    "        axis = 1)\n",
    "\n",
    "\n",
    "# B) # C) creating number of viol and cummulative violations till date for 5 most recent violations ocurred  \n",
    "viol_df['dummy'] = 1\n",
    "pivot_viol_df = pd.pivot_table(viol_df, index=['block_lot','date',], columns=['NOV Category Description'], values=[\"dummy\"], aggfunc=np.sum)\n",
    "pivot_viol_df = pivot_viol_df.reset_index(drop = False)\n",
    "pivot_viol_df.columns = pivot_viol_df.columns.droplevel(0)\n",
    "pivot_viol_df.columns.values[[0, 1]] = ['block_lot', 'date']\n",
    "\n",
    "for i in viol_df['NOV Category Description'].unique():\n",
    "    print(i)\n",
    "    pivot_viol_df['N_{}_violations_current'.format(i)] = pivot_viol_df[i]\n",
    "    pivot_viol_df['N_{}_violations_previous'.format(i)] = pivot_viol_df.groupby(['block_lot'])[i].shift(+1)\n",
    "    pivot_viol_df['N_{}_violations_previous_shift_one'.format(i)] = pivot_viol_df.groupby(['block_lot'])[i].shift(+2)\n",
    "    pivot_viol_df['N_{}_violations_previous_shift_two'.format(i)] = pivot_viol_df.groupby(['block_lot'])[i].shift(+3)\n",
    "    pivot_viol_df['N_{}_violations_previous_shift_three'.format(i)] = pivot_viol_df.groupby(['block_lot'])[i].shift(+4)\n",
    "    pivot_viol_df['N_{}_violations_previous_shift_four'.format(i)] = pivot_viol_df.groupby(['block_lot'])[i].shift(+5)\n",
    "\n",
    "    pivot_viol_df['{}_cumsum_till_date'.format(i)] = pivot_viol_df.groupby(['block_lot'])['N_{}_violations_current'.format(i)].cumsum()\n",
    "    pivot_viol_df['{}_cumsum_till_date_shift'.format(i)] = pivot_viol_df.groupby(['block_lot'])['N_{}_violations_previous'.format(i)].cumsum()\n",
    "    pivot_viol_df['{}_cumsum_till_date_shift_two'.format(i)] = pivot_viol_df.groupby(['block_lot'])['N_{}_violations_previous_shift_one'.format(i)].cumsum()\n",
    "    pivot_viol_df['{}_cumsum_till_date_shift_three'.format(i)] = pivot_viol_df.groupby(['block_lot'])['N_{}_violations_previous_shift_two'.format(i)].cumsum()\n",
    "    pivot_viol_df['{}_cumsum_till_date_shift_four'.format(i)] = pivot_viol_df.groupby(['block_lot'])['N_{}_violations_previous_shift_three'.format(i)].cumsum()\n",
    "    \n",
    "    pivot_viol_df['{}_cumsum_till_date'.format(i)] = pivot_viol_df.groupby(['block_lot'])['{}_cumsum_till_date'.format(i)].ffill()\n",
    "    pivot_viol_df['{}_cumsum_till_date_shift'.format(i)] = pivot_viol_df.groupby(['block_lot'])['{}_cumsum_till_date_shift'.format(i)].ffill()\n",
    "    pivot_viol_df['{}_cumsum_till_date_shift_two'.format(i)] = pivot_viol_df.groupby(['block_lot'])['{}_cumsum_till_date_shift_two'.format(i)].ffill()\n",
    "    pivot_viol_df['{}_cumsum_till_date_shift_three'.format(i)] = pivot_viol_df.groupby(['block_lot'])['{}_cumsum_till_date_shift_three'.format(i)].ffill()\n",
    "    pivot_viol_df['{}_cumsum_till_date_shift_four'.format(i)] = pivot_viol_df.groupby(['block_lot'])['{}_cumsum_till_date_shift_four'.format(i)].ffill()\n",
    "    \n",
    "\n",
    "    viol_df =  pd.DataFrame.merge(viol_df,\n",
    "                          pivot_viol_df[[\n",
    "                              'block_lot', 'date',\n",
    "                              'N_{}_violations_current'.format(i),\n",
    "\n",
    "                              'N_{}_violations_previous'.format(i),\n",
    "                              'N_{}_violations_previous_shift_one'.format(i),\n",
    "                              'N_{}_violations_previous_shift_two'.format(i),\n",
    "                              'N_{}_violations_previous_shift_three'.format(i),\n",
    "                              'N_{}_violations_previous_shift_four'.format(i),\n",
    "                            '{}_cumsum_till_date'.format(i),\n",
    "                            '{}_cumsum_till_date_shift'.format(i),\n",
    "                            '{}_cumsum_till_date_shift_two'.format(i),\n",
    "                            '{}_cumsum_till_date_shift_three'.format(i),\n",
    "                            '{}_cumsum_till_date_shift_four'.format(i)]],\n",
    "                          on=['block_lot', 'date'],\n",
    "                          how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modelling pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining the target variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating target variable for MODELING SCENARIO A: modelling the possibility of any violation happening in the next 60 days \n",
    "\n",
    "auxiliary_df = viol_df[['block_lot', 'date']].drop_duplicates()\n",
    "auxiliary_df['date_shift'] = auxiliary_df.groupby(['block_lot'])['date'].shift(-1)\n",
    "auxiliary_df['days_difference'] = auxiliary_df['date_shift'] - auxiliary_df['date']  \n",
    "auxiliary_df['days_difference'] = auxiliary_df['days_difference'].dt.days\n",
    "# replacing nans with zeroes. NaN values appear in block_lot -date scenarios where there is no info about the next vilation at the blocklot\n",
    "auxiliary_df['days_difference'] = np.where(auxiliary_df['days_difference'].isna(), 0, auxiliary_df['days_difference'])\n",
    "# creating the target variable for classification\n",
    "auxiliary_df['violation_in_following_120_days'] = np.where(auxiliary_df['days_difference'] <= 120, 1,0)\n",
    "# # removing from modelling most recent scenarios (small number of cases)- for these we are not sure if the violation will happen or not in the following period because this perios is still happening\n",
    "# auxiliary_df = auxiliary_df[auxiliary_df['date'] <= auxiliary_df['date'].max() - timedelta(days=60)]\n",
    "viol_df = viol_df.merge(auxiliary_df[['violation_in_following_120_days', 'block_lot', 'date']], on = ['block_lot', 'date'], how = 'left')\n",
    "\n",
    "\n",
    "# creating target variable for MODELING SCENARIO B: modelling the expected ammount of any type of violations during the next violation-ocurring inspection\n",
    "auxiliary_df=  viol_df.groupby(['block_lot', 'date']).size().reset_index(name='counts')\n",
    "auxiliary_df['N_violations_next_insp'] = auxiliary_df.groupby(['block_lot'])['counts'].shift(-1)\n",
    "# replacing nans with zeroes. NaN values appear in block_lot -date scenarios where there is no info about the next vilation at the blocklot\n",
    "auxiliary_df['N_violations_next_insp'] = np.where(auxiliary_df['N_violations_next_insp'].isna(), 0,\\\n",
    "                                                  auxiliary_df['N_violations_next_insp'])\n",
    "# removing from modelling most recent scenarios (small number of cases)- for these we are not sure if the violation will happen or not in the following period because this perios is still happening\n",
    "auxiliary_df = auxiliary_df[auxiliary_df['date'] <= auxiliary_df['date'].max() - timedelta(days=60)]\n",
    "auxiliary_df[auxiliary_df['block_lot'] == '3547_018B']\n",
    "viol_df = viol_df.merge(auxiliary_df[['N_violations_next_insp', 'block_lot', 'date']], on = ['block_lot', 'date'], how = 'left')\n",
    "\n",
    "# Standardizing the dataframe after all of the feature engineering so far has been done\n",
    "viol_df = viol_df.drop_duplicates(subset = ['block_lot', 'date'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test validation rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing the dataset into scoring, testing, validation and training\n",
    "# time series adjusted k fold cross validation (https://stats.stackexchange.com/a/366288) was not feasible, since we dont have a traditional\n",
    "# scoring data will be the most recent case of each block lot i violations dataset, plus all blocklots which werent present at this dataset\n",
    "\n",
    "\n",
    "# temporarily reverse date order\n",
    "# add the groupby row number\n",
    "viol_df = viol_df.sort_values(by='date', ascending = False)\n",
    "viol_df['ordinality'] = viol_df.groupby(['block_lot']).cumcount()+1\n",
    "viol_df['ordinality']= viol_df['ordinality'][::-1]\n",
    "viol_df[viol_df['block_lot']== '4710_003'].head(20)\n",
    "\n",
    "# returning back the normal ascending sort\n",
    "viol_df = viol_df.sort_values(by='date')\n",
    "\n",
    "# data split rules for first round of model building\n",
    "viol_df['train_test_validation_split'] = np.where(viol_df['ordinality'] == 1, 'score_data', np.nan)\n",
    "viol_df['train_test_validation_split'] = np.where(viol_df['ordinality'] == 2, 'test_data',viol_df['train_test_validation_split'])\n",
    "viol_df['train_test_validation_split'] = np.where(viol_df['ordinality'] == 3, 'validation_data', viol_df['train_test_validation_split'])\n",
    "viol_df['train_test_validation_split'] = np.where(viol_df['ordinality'] > 3, 'train_data', viol_df['train_test_validation_split'] )\n",
    "\n",
    "# data split rules for second round of model building, when the best hyperparameters are known\n",
    "viol_df['train_score_split'] = np.where(viol_df['ordinality'] == 1, 'score_data', np.nan)\n",
    "viol_df['train_score_split'] = np.where(viol_df['ordinality'] != 1, 'train_data',viol_df['train_score_split'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dropping columns not required for modelling\n",
    "cols_to_drop = [\n",
    "    'Block',\n",
    "    'Lot',\n",
    "    'Street Number',\n",
    "    'Street Name',\n",
    "    'Status',\n",
    "    'NOV Category Description',\n",
    "    'Item',\n",
    "    'Neighborhoods - Analysis Boundaries',\n",
    "    'Supervisor District',\n",
    "    'Location',\n",
    "    'date_shift_one',\n",
    "    'date_shift_two',\n",
    "    'date_shift_three',\n",
    "    'date_shift_four',\n",
    "    'date_shift_five',\n",
    "    'dummy',\n",
    "#     'block_lot',\n",
    "    'date',\n",
    "    'ordinality'\n",
    "]\n",
    "viol_df = viol_df.drop(cols_to_drop, axis = 1)\n",
    "\n",
    "# replacing Nans and inf values with zeros. These came up in feature engineering while dividing with zero, NaN and similar\n",
    "viol_df = viol_df.replace(np.nan, 0)\n",
    "viol_df = viol_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "# subsetting datasets for first round of modelling (for optimal model validation purposes)\n",
    "train_data = viol_df[viol_df['train_test_validation_split'] == 'train_data']\n",
    "validation_data = viol_df[viol_df['train_test_validation_split'] == 'validation_data']\n",
    "\n",
    "# subsetting dataset for second round of modelling (for testing purposes)\n",
    "train_data_second_model = viol_df[(viol_df['train_test_validation_split'] == 'train_data')\\\n",
    "                                  | (viol_df['train_test_validation_split'] == 'validation_data')]\n",
    "test_data = viol_df[viol_df['train_test_validation_split'] == 'test_data']\n",
    "\n",
    "\n",
    "# subsetting datasets for final model (for final scoring purposes)\n",
    "train_data_final_model = viol_df[viol_df['train_score_split'] == 'train_data']\n",
    "score_data_final_model = viol_df[viol_df['train_score_split'] == 'score_data']\n",
    "\n",
    "# seeing shapes \n",
    "print(\"viol_df\", viol_df.shape)\n",
    "print(\"train_data first model\",train_data.shape)\n",
    "print(\"train_data_second_model\",train_data_second_model.shape)\n",
    "print(\"train_data_final_model\",train_data_final_model.shape)\n",
    "print(\"validation_data\",validation_data.shape)\n",
    "print(\"test_data\",test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"score_data_final_model\",score_data_final_model.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaling datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying columns for scaling \n",
    "numerical_predictor_features = [\n",
    "    'days_since_last_viol',\n",
    "    'days_since_last_viol_shift',\n",
    "    'days_since_last_viol_shift_two',\n",
    "    'days_since_last_viol_shift_three',\n",
    "    'days_since_last_viol_shift_four',\n",
    "    'N_violations_current_insp',\n",
    "    'N_violations_last_insp',\n",
    "    'N_violations_last_insp_shift',\n",
    "    'N_violations_last_insp_shift_two',\n",
    "    'N_violations_last_insp_shift_three',\n",
    "    'N_violations_last_insp_shift_four',\n",
    "    'violations_cumsum_till_date',\n",
    "    'violations_cumsum_till_date_shift',\n",
    "    'violations_cumsum_till_date_shift_two',\n",
    "    'violations_cumsum_till_date_shift_three',\n",
    "    'violations_cumsum_till_date_shift_four',\n",
    "    'avg_days_between_previous_violations',\n",
    "    'avg_N_violations_recent_inspections',\n",
    "    'StDev_days_between_previous_violations',\n",
    "    'ratio_avg_days_avg_N_violations',\n",
    "    'days_since_last_viol_ratio_other',\n",
    "    'days_since_last_viol_shift_ratio_other',\n",
    "    'days_since_last_viol_shift_two_ratio_other',\n",
    "    'days_since_last_viol_shift_three_ratio_other',\n",
    "    'days_since_last_viol_shift_four_ratio_other',\n",
    "    'N_violations_current_insp_ratio_other',\n",
    "    'N_violations_last_insp_ratio_other',\n",
    "    'N_violations_last_insp_shift_ratio_other',\n",
    "    'N_violations_last_insp_shift_two_ratio_other',\n",
    "    'N_violations_last_insp_shift_three_ratio_other',\n",
    "    'N_violations_last_insp_shift_four_ratio_other',\n",
    "    'days_since_last_viol_subtracted',\n",
    "    'days_since_last_viol_shift_subtracted',\n",
    "    'days_since_last_viol_shift_two_subtracted',\n",
    "    'days_since_last_viol_shift_three_subtracted',\n",
    "    'AVG_days_since_columnwise_diff',\n",
    "    'StDev_days_since_columnwise_diff',\n",
    "    'N_violations_current_insp_subtracted',\n",
    "    'N_violations_last_insp_subtracted',\n",
    "    'N_violations_last_insp_shift_subtracted',\n",
    "    'N_violations_last_insp_shift_two_subtracted',\n",
    "    'N_violations_last_insp_shift_three_subtracted',\n",
    "    'AVG_N_violations_columnwise_diff',\n",
    "    'StDev_N_violations_columnwise_diff',\n",
    "    'days_since_last_viol_ratio_previous',\n",
    "    'days_since_last_viol_shift_ratio_previous',\n",
    "    'days_since_last_viol_shift_two_ratio_previous',\n",
    "    'days_since_last_viol_shift_three_ratio_previous',\n",
    "    'AVG_days_since_columnwise_ratio',\n",
    "    'StDev_days_since_columnwise_ratio',\n",
    "    'N_violations_current_insp_ratio_previous',\n",
    "    'N_violations_last_insp_ratio_previous',\n",
    "    'N_violations_last_insp_shift_ratio_previous',\n",
    "    'N_violations_last_insp_shift_two_ratio_previous',\n",
    "    'N_violations_last_insp_shift_three_ratio_previous',\n",
    "    'AVG_N_violations_columnwise_ratio',\n",
    "    'StDev_N_violations_columnwise_ratio',\n",
    "    'violations_cumsum_till_date_ratio_previous',\n",
    "    'violations_cumsum_till_date_shift_ratio_previous',\n",
    "    'violations_cumsum_till_date_shift_two_ratio_previous',\n",
    "    'violations_cumsum_till_date_shift_three_ratio_previous',\n",
    "    'AVG_cumsum_columnwise_ratio',\n",
    "    'StDev_cumsum_columnwise_ratio',\n",
    "    'ratio_days_since_N_violations_last_insp',\n",
    "    'ratio_days_since_N_violations_last_insp_shift',\n",
    "    'ratio_days_since_N_violations_last_insp_shift_two',\n",
    "    'ratio_days_since_N_violations_last_insp_shift_three',\n",
    "    'ratio_days_since_N_violations_last_insp_shift_four',\n",
    "    'AVG_columnwise_ratio_days_since_N_violations',\n",
    "    'StDev_columnwise_ratio_days_since_N_violations',\n",
    "    'days_since_building section',\n",
    "    'days_since_building section_shift',\n",
    "    'days_since_building section_shift_two',\n",
    "    'days_since_building section_shift_three',\n",
    "    'days_since_building section_shift_four',\n",
    "    'days_since_other section',\n",
    "    'days_since_other section_shift',\n",
    "    'days_since_other section_shift_two',\n",
    "    'days_since_other section_shift_three',\n",
    "    'days_since_other section_shift_four',\n",
    "    'days_since_fire section',\n",
    "    'days_since_fire section_shift',\n",
    "    'days_since_fire section_shift_two',\n",
    "    'days_since_fire section_shift_three',\n",
    "    'days_since_fire section_shift_four',\n",
    "    'days_since_sanitation section',\n",
    "    'days_since_sanitation section_shift',\n",
    "    'days_since_sanitation section_shift_two',\n",
    "    'days_since_sanitation section_shift_three',\n",
    "    'days_since_sanitation section_shift_four',\n",
    "    'days_since_interior surfaces section',\n",
    "    'days_since_interior surfaces section_shift',\n",
    "    'days_since_interior surfaces section_shift_two',\n",
    "    'days_since_interior surfaces section_shift_three',\n",
    "    'days_since_interior surfaces section_shift_four',\n",
    "    'days_since_plumbing and electrical section',\n",
    "    'days_since_plumbing and electrical section_shift',\n",
    "    'days_since_plumbing and electrical section_shift_two',\n",
    "    'days_since_plumbing and electrical section_shift_three',\n",
    "    'days_since_plumbing and electrical section_shift_four',\n",
    "    'days_since_security requirements section',\n",
    "    'days_since_security requirements section_shift',\n",
    "    'days_since_security requirements section_shift_two',\n",
    "    'days_since_security requirements section_shift_three',\n",
    "    'days_since_security requirements section_shift_four',\n",
    "    'days_since_smoke detection section',\n",
    "    'days_since_smoke detection section_shift',\n",
    "    'days_since_smoke detection section_shift_two',\n",
    "    'days_since_smoke detection section_shift_three',\n",
    "    'days_since_smoke detection section_shift_four',\n",
    "    'days_since_lead section',\n",
    "    'days_since_lead section_shift',\n",
    "    'days_since_lead section_shift_two',\n",
    "    'days_since_lead section_shift_three',\n",
    "    'days_since_lead section_shift_four',\n",
    "    'days_since_hco',\n",
    "    'days_since_hco_shift',\n",
    "    'days_since_hco_shift_two',\n",
    "    'days_since_hco_shift_three',\n",
    "    'days_since_hco_shift_four',\n",
    "    'N_building section_violations_current',\n",
    "    'N_building section_violations_previous',\n",
    "    'N_building section_violations_previous_shift_one',\n",
    "    'N_building section_violations_previous_shift_two',\n",
    "    'N_building section_violations_previous_shift_three',\n",
    "    'N_building section_violations_previous_shift_four',\n",
    "    'building section_cumsum_till_date',\n",
    "    'building section_cumsum_till_date_shift',\n",
    "    'building section_cumsum_till_date_shift_two',\n",
    "    'building section_cumsum_till_date_shift_three',\n",
    "    'building section_cumsum_till_date_shift_four',\n",
    "    'N_other section_violations_current',\n",
    "    'N_other section_violations_previous',\n",
    "    'N_other section_violations_previous_shift_one',\n",
    "    'N_other section_violations_previous_shift_two',\n",
    "    'N_other section_violations_previous_shift_three',\n",
    "    'N_other section_violations_previous_shift_four',\n",
    "    'other section_cumsum_till_date',\n",
    "    'other section_cumsum_till_date_shift',\n",
    "    'other section_cumsum_till_date_shift_two',\n",
    "    'other section_cumsum_till_date_shift_three',\n",
    "    'other section_cumsum_till_date_shift_four',\n",
    "    'N_fire section_violations_current',\n",
    "    'N_fire section_violations_previous',\n",
    "    'N_fire section_violations_previous_shift_one',\n",
    "    'N_fire section_violations_previous_shift_two',\n",
    "    'N_fire section_violations_previous_shift_three',\n",
    "    'N_fire section_violations_previous_shift_four',\n",
    "    'fire section_cumsum_till_date',\n",
    "    'fire section_cumsum_till_date_shift',\n",
    "    'fire section_cumsum_till_date_shift_two',\n",
    "    'fire section_cumsum_till_date_shift_three',\n",
    "    'fire section_cumsum_till_date_shift_four',\n",
    "    'N_sanitation section_violations_current',\n",
    "    'N_sanitation section_violations_previous',\n",
    "    'N_sanitation section_violations_previous_shift_one',\n",
    "    'N_sanitation section_violations_previous_shift_two',\n",
    "    'N_sanitation section_violations_previous_shift_three',\n",
    "    'N_sanitation section_violations_previous_shift_four',\n",
    "    'sanitation section_cumsum_till_date',\n",
    "    'sanitation section_cumsum_till_date_shift',\n",
    "    'sanitation section_cumsum_till_date_shift_two',\n",
    "    'sanitation section_cumsum_till_date_shift_three',\n",
    "    'sanitation section_cumsum_till_date_shift_four',\n",
    "    'N_interior surfaces section_violations_current',\n",
    "    'N_interior surfaces section_violations_previous',\n",
    "    'N_interior surfaces section_violations_previous_shift_one',\n",
    "    'N_interior surfaces section_violations_previous_shift_two',\n",
    "    'N_interior surfaces section_violations_previous_shift_three',\n",
    "    'N_interior surfaces section_violations_previous_shift_four',\n",
    "    'interior surfaces section_cumsum_till_date',\n",
    "    'interior surfaces section_cumsum_till_date_shift',\n",
    "    'interior surfaces section_cumsum_till_date_shift_two',\n",
    "    'interior surfaces section_cumsum_till_date_shift_three',\n",
    "    'interior surfaces section_cumsum_till_date_shift_four',\n",
    "    'N_plumbing and electrical section_violations_current',\n",
    "    'N_plumbing and electrical section_violations_previous',\n",
    "    'N_plumbing and electrical section_violations_previous_shift_one',\n",
    "    'N_plumbing and electrical section_violations_previous_shift_two',\n",
    "    'N_plumbing and electrical section_violations_previous_shift_three',\n",
    "    'N_plumbing and electrical section_violations_previous_shift_four',\n",
    "    'plumbing and electrical section_cumsum_till_date',\n",
    "    'plumbing and electrical section_cumsum_till_date_shift',\n",
    "    'plumbing and electrical section_cumsum_till_date_shift_two',\n",
    "    'plumbing and electrical section_cumsum_till_date_shift_three',\n",
    "    'plumbing and electrical section_cumsum_till_date_shift_four',\n",
    "    'N_security requirements section_violations_current',\n",
    "    'N_security requirements section_violations_previous',\n",
    "    'N_security requirements section_violations_previous_shift_one',\n",
    "    'N_security requirements section_violations_previous_shift_two',\n",
    "    'N_security requirements section_violations_previous_shift_three',\n",
    "    'N_security requirements section_violations_previous_shift_four',\n",
    "    'security requirements section_cumsum_till_date',\n",
    "    'security requirements section_cumsum_till_date_shift',\n",
    "    'security requirements section_cumsum_till_date_shift_two',\n",
    "    'security requirements section_cumsum_till_date_shift_three',\n",
    "    'security requirements section_cumsum_till_date_shift_four',\n",
    "    'N_smoke detection section_violations_current',\n",
    "    'N_smoke detection section_violations_previous',\n",
    "    'N_smoke detection section_violations_previous_shift_one',\n",
    "    'N_smoke detection section_violations_previous_shift_two',\n",
    "    'N_smoke detection section_violations_previous_shift_three',\n",
    "    'N_smoke detection section_violations_previous_shift_four',\n",
    "    'smoke detection section_cumsum_till_date',\n",
    "    'smoke detection section_cumsum_till_date_shift',\n",
    "    'smoke detection section_cumsum_till_date_shift_two',\n",
    "    'smoke detection section_cumsum_till_date_shift_three',\n",
    "    'smoke detection section_cumsum_till_date_shift_four',\n",
    "    'N_lead section_violations_current',\n",
    "    'N_lead section_violations_previous',\n",
    "    'N_lead section_violations_previous_shift_one',\n",
    "    'N_lead section_violations_previous_shift_two',\n",
    "    'N_lead section_violations_previous_shift_three',\n",
    "    'N_lead section_violations_previous_shift_four',\n",
    "    'lead section_cumsum_till_date',\n",
    "    'lead section_cumsum_till_date_shift',\n",
    "    'lead section_cumsum_till_date_shift_two',\n",
    "    'lead section_cumsum_till_date_shift_three',\n",
    "    'lead section_cumsum_till_date_shift_four',\n",
    "    'N_hco_violations_current',\n",
    "    'N_hco_violations_previous',\n",
    "    'N_hco_violations_previous_shift_one',\n",
    "    'N_hco_violations_previous_shift_two',\n",
    "    'N_hco_violations_previous_shift_three',\n",
    "    'N_hco_violations_previous_shift_four',\n",
    "    'hco_cumsum_till_date',\n",
    "    'hco_cumsum_till_date_shift',\n",
    "    'hco_cumsum_till_date_shift_two',\n",
    "    'hco_cumsum_till_date_shift_three',\n",
    "    'hco_cumsum_till_date_shift_four']\n",
    "\n",
    "\n",
    "# SCALING DATA \n",
    "# first model\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train_data.loc[:, numerical_predictor_features])\n",
    "train_scaled_features = scaler.transform(train_data.loc[:, numerical_predictor_features])\n",
    "train_scaled_features = pd.DataFrame(train_scaled_features)\n",
    "train_scaled_features.columns = numerical_predictor_features\n",
    "train_data = train_data.drop(numerical_predictor_features, axis=1).reset_index(level=0, drop=True)\n",
    "train_data = pd.concat((train_data, train_scaled_features), axis=1)\n",
    "\n",
    "validation_scaled_features = scaler.transform(validation_data.loc[:, numerical_predictor_features])\n",
    "validation_scaled_features = pd.DataFrame(validation_scaled_features)\n",
    "validation_scaled_features.columns = numerical_predictor_features\n",
    "validation_data = validation_data.drop(numerical_predictor_features, axis=1).reset_index(level=0, drop=True)\n",
    "validation_data = pd.concat((validation_data, validation_scaled_features), axis=1)\n",
    "\n",
    "\n",
    "# second model\n",
    "scaler_second_model = StandardScaler()\n",
    "scaler_second_model = scaler_second_model.fit(train_data_second_model.loc[:, numerical_predictor_features])\n",
    "train_second_scaled_features = scaler_second_model.transform(train_data_second_model.loc[:, numerical_predictor_features])\n",
    "train_second_scaled_features = pd.DataFrame(train_second_scaled_features)\n",
    "train_second_scaled_features.columns = numerical_predictor_features\n",
    "train_data_second_model = train_data_second_model.drop(numerical_predictor_features, axis=1).reset_index(level=0, drop=True)\n",
    "train_data_second_model = pd.concat((train_data_second_model, train_second_scaled_features), axis=1)\n",
    "\n",
    "test_scaled_features = scaler_second_model.transform(test_data.loc[:, numerical_predictor_features])\n",
    "test_scaled_features = pd.DataFrame(test_scaled_features)\n",
    "test_scaled_features.columns = numerical_predictor_features\n",
    "test_data = test_data.drop(numerical_predictor_features, axis=1).reset_index(level=0, drop=True)\n",
    "test_data = pd.concat((test_data, test_scaled_features), axis=1)\n",
    "\n",
    "\n",
    "# final model\n",
    "scaler_final_model = StandardScaler()\n",
    "scaler_final_model = scaler_final_model.fit(train_data_final_model.loc[:, numerical_predictor_features])\n",
    "train_final_scaled_features = scaler_final_model.transform(train_data_final_model.loc[:, numerical_predictor_features])\n",
    "train_final_scaled_features = pd.DataFrame(train_final_scaled_features)\n",
    "train_final_scaled_features.columns = numerical_predictor_features\n",
    "train_data_final_model = train_data_final_model.drop(numerical_predictor_features, axis=1).reset_index(level=0, drop=True)\n",
    "train_data_final_model = pd.concat((train_data_final_model, train_final_scaled_features), axis=1)\n",
    "\n",
    "score_final_scaled_features = scaler_final_model.transform(score_data_final_model.loc[:, numerical_predictor_features])\n",
    "score_final_scaled_features = pd.DataFrame(score_final_scaled_features)\n",
    "score_final_scaled_features.columns = numerical_predictor_features\n",
    "score_data_final_model = score_data_final_model.drop(numerical_predictor_features, axis=1).reset_index(level=0, drop=True)\n",
    "score_data_final_model = pd.concat((score_data_final_model, score_final_scaled_features), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model - choosing the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost\n",
    "\n",
    "\n",
    "# define predictor and target feature. So far only categorical features are in the model\n",
    "predictor_features = numerical_predictor_features\n",
    "target_feature = 'violation_in_following_120_days'\n",
    "\n",
    "# create parameter grid\n",
    "xgb_param_grid ={\n",
    "    'nthread': [1],  # use maximum number of threads\n",
    "    'objective': ['binary:logistic'],\n",
    "    \"grow_policy\": [\"lossguide\"],\n",
    "    \"num_class\": [1],\n",
    "    \"tree_method\": [\"hist\"],\n",
    "    \"max_depth\": [5,8,9,12,15], \n",
    "    \"min_child_weight\": [2,4,6,8],  \n",
    "    \"colsample_bytree\": [0.7,0.75,0.8,0.85],\n",
    "    \"gamma\": [0, 1],\n",
    "    \"subsample\": [0.7,0.8,0.9,0.95],\n",
    "    'learning_rate': [0.01, 0.07, 0.1],\n",
    "    'silent': [1],\n",
    "    'seed': [1337],\n",
    "    \"n_estimators\": [10, 15, 20,40,60]\n",
    "    }\n",
    "\n",
    "\n",
    "# choosing the first model\n",
    "best_score = 0\n",
    "best_params = None\n",
    "for param in ParameterGrid(xgb_param_grid):\n",
    "    # fit the model\n",
    "    model = xgboost.XGBClassifier(**param)\n",
    "    model.fit(train_data[predictor_features],train_data[target_feature])\n",
    "    \n",
    "    # get auc\n",
    "    predicted_values = model.predict_proba(validation_data[predictor_features])[:,1]\n",
    "    auc = roc_auc_score(validation_data['violation_in_following_120_days'], predicted_values)\n",
    "    \n",
    "\n",
    "    # save if best\n",
    "    if auc > best_score:\n",
    "        best_score = auc\n",
    "        best_params = model.get_xgb_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second model - evaulation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBClassifier(**best_params)\n",
    "model.fit(train_data_second_model[predictor_features],train_data_second_model[target_feature])\n",
    "predicted_values = model.predict_proba(test_data[predictor_features])[:,1]\n",
    "auc = roc_auc_score(test_data['violation_in_following_120_days'], predicted_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score data\n",
    "scored_values = model.predict_proba(score_data_final_model[predictor_features])[:,1] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test auc\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_data['violation_in_following_120_days'], predicted_values)\n",
    "accuracy_scores = []\n",
    "for thresh in thresholds:\n",
    "    accuracy_scores.append(accuracy_score(test_data['violation_in_following_120_days'], \n",
    "                                         [1 if m > thresh else 0 for m in predicted_values]))\n",
    "\n",
    "accuracies = np.array(accuracy_scores)\n",
    "max_accuracy = accuracies.max() \n",
    "max_accuracy_threshold =  thresholds[accuracies.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost\n",
    "\n",
    "\n",
    "# define predictor and target feature. So far only categorical features are in the model\n",
    "predictor_features = numerical_predictor_features\n",
    "target_feature = 'N_violations_next_insp'\n",
    "# create parameter grid\n",
    "xgb_param_grid ={\n",
    "    'nthread': [1],  # use maximum number of threads\n",
    "    'objective': ['reg:squarederror'],\n",
    "    \"grow_policy\": [\"lossguide\"],\n",
    "    # \"num_class\": [1],\n",
    "    \"tree_method\": [\"hist\"],\n",
    "    \"max_depth\": [4,5,6,7,8], \n",
    "    \"min_child_weight\": [2,4,6,8],  \n",
    "    \"colsample_bytree\": [0.7,0.75,0.8,0.85],\n",
    "    \"gamma\": [0, 1],\n",
    "    \"subsample\": [0.7,0.8,0.9,0.95],\n",
    "    'learning_rate': [0.01, 0.07, 0.1],\n",
    "    'silent': [1],\n",
    "    'seed': [1337],\n",
    "    \"n_estimators\": [10, 15, 20,40,60]\n",
    "    }\n",
    "\n",
    "\n",
    "# choosing the first model\n",
    "regressor_best_score = 1000\n",
    "regressor_best_params = None\n",
    "for param in ParameterGrid(xgb_param_grid):\n",
    "    # fit the model\n",
    "    model = xgboost.XGBRegressor(**param)\n",
    "    model.fit(train_data[predictor_features],train_data[target_feature])\n",
    "    \n",
    "    # get auc\n",
    "    predicted_values = model.predict(validation_data[predictor_features])\n",
    "    mae = mean_absolute_error(validation_data[target_feature], predicted_values)\n",
    "    \n",
    "\n",
    "    # save if best\n",
    "    if mae < regressor_best_score:\n",
    "        regressor_best_score = mae\n",
    "        regressor_best_params = model.get_xgb_params()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_model = xgboost.XGBRegressor(**regressor_best_params)\n",
    "regressor_model.fit(train_data_second_model[predictor_features],train_data_second_model[target_feature])\n",
    "regressor_predicted_values = regressor_model.predict(test_data[predictor_features])\n",
    "mae = mean_absolute_error(test_data[target_feature], regressor_predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test mae\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data[target_feature].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
