{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideOutput": false
   },
   "source": [
    "# VIOLATIONS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_data = pd.read_csv('/home/deploy/notebooks/city_violations_data/input/SF_download/violations/Notices_of_violation_SF.csv',\n",
    "                sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(387095, 19)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viol_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrangling some columns\n",
    "\n",
    "# converting column to date\n",
    "viol_data['date'] = pd.to_datetime(viol_data['Date Filed'], format='%m/%d/%Y')\n",
    "\n",
    "# creating column which will be a basis for prediction - block_lot (parcel)\n",
    "viol_data['block_lot'] = viol_data['Block'].astype(str) + '_' + viol_data['Lot'].astype(str)\n",
    "viol_data['blklot'] = viol_data['Block'].astype(str) + viol_data['Lot'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_data.drop(['Date Filed','Complaint Number', 'Item Sequence Number', 'Street Suffix', 'Unit','Receiving Division','Assigned Division',\n",
    "        'Zipcode', 'NOV Item Description',  ], axis =1 , inplace = True   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Block</th>\n",
       "      <th>Lot</th>\n",
       "      <th>Street Number</th>\n",
       "      <th>Street Name</th>\n",
       "      <th>Status</th>\n",
       "      <th>NOV Category Description</th>\n",
       "      <th>Item</th>\n",
       "      <th>Neighborhoods - Analysis Boundaries</th>\n",
       "      <th>Supervisor District</th>\n",
       "      <th>Location</th>\n",
       "      <th>date</th>\n",
       "      <th>block_lot</th>\n",
       "      <th>blklot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0581</td>\n",
       "      <td>013</td>\n",
       "      <td>2632</td>\n",
       "      <td>Fillmore</td>\n",
       "      <td>not active</td>\n",
       "      <td>fire section</td>\n",
       "      <td>provide fire extinguisher type 2a 10bc or equi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-04-21</td>\n",
       "      <td>0581_013</td>\n",
       "      <td>0581013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3532</td>\n",
       "      <td>015A</td>\n",
       "      <td>322</td>\n",
       "      <td>14th</td>\n",
       "      <td>not active</td>\n",
       "      <td>interior surfaces section</td>\n",
       "      <td>remove all garbage trash from rear steps sec10...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001-09-24</td>\n",
       "      <td>3532_015A</td>\n",
       "      <td>3532015A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3631</td>\n",
       "      <td>016</td>\n",
       "      <td>995</td>\n",
       "      <td>Dolores</td>\n",
       "      <td>not active</td>\n",
       "      <td>building section</td>\n",
       "      <td>this notice includes violations for the areas ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2001-07-13</td>\n",
       "      <td>3631_016</td>\n",
       "      <td>3631016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0517</td>\n",
       "      <td>010</td>\n",
       "      <td>116</td>\n",
       "      <td>29th</td>\n",
       "      <td>not active</td>\n",
       "      <td>building section</td>\n",
       "      <td>repair window glazing seal (1001(h),(j),&amp; 708 hc)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1999-11-18</td>\n",
       "      <td>0517_010</td>\n",
       "      <td>0517010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1433</td>\n",
       "      <td>023</td>\n",
       "      <td>348</td>\n",
       "      <td>02nd</td>\n",
       "      <td>not active</td>\n",
       "      <td>other section</td>\n",
       "      <td>other fire violations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002-05-06</td>\n",
       "      <td>1433_023</td>\n",
       "      <td>1433023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Block   Lot  Street Number Street Name      Status  \\\n",
       "0  0581   013           2632    Fillmore  not active   \n",
       "1  3532  015A            322        14th  not active   \n",
       "2  3631   016            995     Dolores  not active   \n",
       "3  0517   010            116        29th  not active   \n",
       "4  1433   023            348        02nd  not active   \n",
       "\n",
       "    NOV Category Description  \\\n",
       "0               fire section   \n",
       "1  interior surfaces section   \n",
       "2           building section   \n",
       "3           building section   \n",
       "4              other section   \n",
       "\n",
       "                                                Item  \\\n",
       "0  provide fire extinguisher type 2a 10bc or equi...   \n",
       "1  remove all garbage trash from rear steps sec10...   \n",
       "2  this notice includes violations for the areas ...   \n",
       "3  repair window glazing seal (1001(h),(j),& 708 hc)   \n",
       "4                              other fire violations   \n",
       "\n",
       "  Neighborhoods - Analysis Boundaries  Supervisor District Location  \\\n",
       "0                                 NaN                  NaN      NaN   \n",
       "1                                 NaN                  NaN      NaN   \n",
       "2                                 NaN                  NaN      NaN   \n",
       "3                                 NaN                  NaN      NaN   \n",
       "4                                 NaN                  NaN      NaN   \n",
       "\n",
       "        date  block_lot    blklot  \n",
       "0 2008-04-21   0581_013   0581013  \n",
       "1 2001-09-24  3532_015A  3532015A  \n",
       "2 2001-07-13   3631_016   3631016  \n",
       "3 1999-11-18   0517_010   0517010  \n",
       "4 2002-05-06   1433_023   1433023  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viol_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3703028    1197\n",
       "0340012     959\n",
       "3725066     872\n",
       "3553022     828\n",
       "3505001     819\n",
       "           ... \n",
       "0839035       1\n",
       "0842014       1\n",
       "4094030       1\n",
       "3553036       1\n",
       "0123035       1\n",
       "Name: blklot, Length: 24700, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viol_data['blklot'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block: 3610\n",
      "Lot: 586\n",
      "Street Number: 4366\n",
      "Street Name: 1247\n",
      "Status: 2\n",
      "NOV Category Description: 10\n",
      "Item: 51445\n",
      "Neighborhoods - Analysis Boundaries: 42\n",
      "Supervisor District: 12\n",
      "Location: 23858\n",
      "date: 6022\n",
      "block_lot: 24700\n"
     ]
    }
   ],
   "source": [
    "for col in viol_data.columns:\n",
    "    print('{}: {}'.format(col, len(viol_data[col].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0\n",
      "Lot 0\n",
      "Street Number 0\n",
      "Street Name 0\n",
      "Status 0\n",
      "NOV Category Description 0\n",
      "Item 48\n",
      "Neighborhoods - Analysis Boundaries 2945\n",
      "Supervisor District 2945\n",
      "Location 2919\n",
      "date 0\n",
      "block_lot 0\n"
     ]
    }
   ],
   "source": [
    "for col in viol_data.columns:\n",
    "    print(col, viol_data[col].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (2,14,15,16,17,18,19,20,21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/pyproj/crs/crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "# step -1 -- add parcel data and include this in the dataset also\n",
    "parcel_df = pd.read_csv('/home/deploy/notebooks/city_violations_data/input/SF_download/parcels/Parcels___Active_and_Retired.csv',\n",
    "                sep = ',')\n",
    "\n",
    "# calculating the centroids\n",
    "geometry = parcel_df['shape'].map(shapely.wkt.loads)\n",
    "crs = {'init': 'epsg:4326'}\n",
    "gdf = gpd.GeoDataFrame(parcel_df, crs=crs, geometry=geometry)\n",
    "gdf['Centroids_location'] = gdf['geometry'].centroid\n",
    "\n",
    "# merging the centroids to the violations data\n",
    "viol_data = viol_data.merge(gdf[['blklot','Centroids_location']], on = ['blklot'], how = 'outer')\n",
    "\n",
    "# assigning coordinates from viol data in cases some values were nan after joining. Centroid locatons will be used for feature engineering\n",
    "viol_data['Centroids_location'] = np.where(viol_data['Centroids_location'].isna(), viol_data['Location'], viol_data['Centroids_location'])\n",
    "\n",
    "del geometry\n",
    "del gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(592825, 14)\n",
      "2919\n"
     ]
    }
   ],
   "source": [
    "print(viol_data.shape)\n",
    "print(viol_data['Centroids_location'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Block  Lot  Street Number Street Name      Status  \\\n",
      "68424  1267  011            114        Carl  not active   \n",
      "\n",
      "      NOV Category Description                Item  \\\n",
      "68424            other section  inspector comments   \n",
      "\n",
      "      Neighborhoods - Analysis Boundaries  Supervisor District  \\\n",
      "68424                      Haight Ashbury                  5.0   \n",
      "\n",
      "                                            Location       date block_lot  \\\n",
      "68424  POINT (-122.45060378672504 37.76600453097519) 2005-06-06  1267_011   \n",
      "\n",
      "        blklot  \n",
      "68424  1267011  \n",
      "Empty DataFrame\n",
      "Columns: [Block, Lot, Street Number, Street Name, Status, NOV Category Description, Item, Neighborhoods - Analysis Boundaries, Supervisor District, Location, date, block_lot, blklot]\n",
      "Index: []\n",
      "      Block  Lot  Street Number Street Name      Status  \\\n",
      "68424  1267  011          114.0        Carl  not active   \n",
      "68424   NaN  NaN            NaN         NaN         NaN   \n",
      "68424   NaN  NaN            NaN         NaN         NaN   \n",
      "\n",
      "      NOV Category Description                Item  \\\n",
      "68424            other section  inspector comments   \n",
      "68424                    dummy                 NaN   \n",
      "68424                    dummy                 NaN   \n",
      "\n",
      "      Neighborhoods - Analysis Boundaries  Supervisor District  \\\n",
      "68424                      Haight Ashbury                  5.0   \n",
      "68424                                 NaN                  NaN   \n",
      "68424                                 NaN                  NaN   \n",
      "\n",
      "                                            Location       date block_lot  \\\n",
      "68424  POINT (-122.45060378672504 37.76600453097519) 2005-06-06  1267_011   \n",
      "68424                                            NaN 1993-10-08       NaN   \n",
      "68424                                            NaN 2020-04-01       NaN   \n",
      "\n",
      "        blklot  \n",
      "68424  1267011  \n",
      "68424  1267011  \n",
      "68424  1267011  \n",
      "Empty DataFrame\n",
      "Columns: [Block, Lot, Street Number, Street Name, Status, NOV Category Description, Item, Neighborhoods - Analysis Boundaries, Supervisor District, Location, date, block_lot, blklot]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# adding dummy rows for earliest and latest date in the dataset - these are used as starting and end point for all block lots resampling\n",
    "\n",
    "earliest_date = viol_data['date'].min().date()\n",
    "latest_date = viol_data['date'].max().date()\n",
    "\n",
    "\n",
    "# these will be excluded from concatenation\n",
    "earliest_dates_blocklots = list(set(viol_data[viol_data['date']==str(earliest_date)]['blklot'].values))\n",
    "latest_dates_blocklots = list(set(viol_data[viol_data['date']==str(latest_date)]['blklot'].values))\n",
    "\n",
    "dummy_start_dates = viol_data[['blklot', 'date']].drop_duplicates(['blklot'])\n",
    "dummy_start_dates = dummy_start_dates[-dummy_start_dates['blklot'].isin(earliest_dates_blocklots)]\n",
    "dummy_start_dates['date'] = earliest_date\n",
    "\n",
    "dummy_end_dates = viol_data[['blklot', 'date']].drop_duplicates(['blklot'])\n",
    "dummy_end_dates = dummy_end_dates[-dummy_end_dates['blklot'].isin(latest_dates_blocklots)]\n",
    "dummy_end_dates['date'] = latest_date\n",
    "\n",
    "dummy_start_dates\n",
    "# sanity check for insertion of dates\n",
    "print(viol_data[viol_data['blklot'] == '1267011'])\n",
    "print(viol_data[viol_data['blklot'] == '4071052'])\n",
    "\n",
    "# concatenating dummy columns for standardizing and reshaping purposes\n",
    "viol_data = pd.concat([viol_data, dummy_start_dates, dummy_end_dates], axis = 0)\n",
    "\n",
    "# converting back to datetime  because it gets wrongly formatted after concatenation\n",
    "viol_data['date'] = pd.to_datetime(viol_data['date'])\n",
    "viol_data['NOV Category Description'] = viol_data['NOV Category Description'].replace(np.nan, 'dummy')\n",
    "\n",
    "# drop nans in date column which were created as a result of merging the dataframes\n",
    "viol_data = viol_data.dropna(subset= ['date'])\n",
    "\n",
    "# sanity check for insertion of dates\n",
    "print(viol_data[viol_data['blklot'] == '1267011'])\n",
    "print(viol_data[viol_data['blklot'] == '4071052'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling to 6 months interval\n",
    "df = viol_data.copy()\n",
    "df = df.sort_values(by='date')\n",
    "df = df.set_index('date')\n",
    "df = df[['blklot','NOV Category Description']]\n",
    "df = df.groupby(['blklot', 'NOV Category Description']).resample('Q', convention = 'end').agg('count')\n",
    "df = df.unstack(level = 1)\n",
    "df.reset_index(inplace=True) \n",
    "df.reset_index(drop = True)\n",
    "df.columns = df.columns.droplevel()\n",
    "df.columns.values[[0, 1]] = ['blklot', 'date']\n",
    "df = df.loc[:,~df.columns.duplicated()]\n",
    "df = df.drop(['dummy'], axis = 1)\n",
    "\n",
    "# resampling by using method '2Q' or '6M' was giving problematic results, therefore below is a workaround by resampling the already resampled dataset\n",
    "df = df.set_index('date')\n",
    "df = df.groupby(['blklot']).resample('2Q', convention = 'end').agg('sum')\n",
    "df.reset_index(inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create overall sums\n",
    "df['N_violations_current_quarter'] = df[['building section', 'fire section', 'hco',\n",
    "       'interior surfaces section', 'lead section', 'other section',\n",
    "       'plumbing and electrical section', 'sanitation section',\n",
    "       'security requirements section', 'smoke detection section']].sum(axis = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(847951, 14)\n",
      "(230430, 14)\n",
      "(12443220, 13)\n",
      "(12443220, 14)\n"
     ]
    }
   ],
   "source": [
    "# # merge coordinates with the dataset\n",
    "\n",
    "print(viol_data.shape)\n",
    "viol_data = viol_data.drop_duplicates(subset = ['blklot'])\n",
    "print(viol_data.shape)\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "df = df.merge(viol_data[['blklot', 'Centroids_location']], on = ['blklot'], how = 'inner' )\n",
    "print(df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving data after this stip so we dont need to go over it again\n",
    "# df.to_csv('6m_standardized_df_no_parcels_and_centroids.csv', sep = '|')\n",
    "# df = pd.read_csv('6m_standardized_df_parcels_and_cenroids.csv', sep = '|')\n",
    "# df['date']= pd.to_datetime(df['date']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk forward time series modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### group level feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol_df = viol_data.copy()\n",
    "viol_df = viol_df.sort_values(by='date')\n",
    "\n",
    "#################################### TIER I DERIVATED VARIABLES ####################################\n",
    "\n",
    "# A) calculating days since last violations for 3 most recent violations ocurred \n",
    "auxiliary_viol_df = viol_df[['block_lot', 'date']].drop_duplicates()\n",
    "auxiliary_viol_df['date_shift_one'] = auxiliary_viol_df.groupby(['block_lot'])['date'].shift(+1)\n",
    "auxiliary_viol_df['date_shift_two'] = auxiliary_viol_df.groupby(['block_lot'])['date'].shift(+2)\n",
    "auxiliary_viol_df['date_shift_three'] = auxiliary_viol_df.groupby(['block_lot'])['date'].shift(+3)\n",
    "auxiliary_viol_df['date_shift_four'] = auxiliary_viol_df.groupby(['block_lot'])['date'].shift(+4)\n",
    "auxiliary_viol_df['date_shift_five'] = auxiliary_viol_df.groupby(['block_lot'])['date'].shift(+5)\n",
    "\n",
    "viol_df = viol_df.merge(auxiliary_viol_df, on = ['block_lot', 'date'], how = 'left')\n",
    "\n",
    "# calculating timedelta\n",
    "viol_df['days_since_last_viol'] = viol_df['date'] - viol_df['date_shift_one']\n",
    "viol_df['days_since_last_viol_shift'] = viol_df['date'] - viol_df['date_shift_two']\n",
    "viol_df['days_since_last_viol_shift_two'] = viol_df['date'] - viol_df['date_shift_three']\n",
    "viol_df['days_since_last_viol_shift_three'] = viol_df['date'] - viol_df['date_shift_four']\n",
    "viol_df['days_since_last_viol_shift_four'] = viol_df['date'] - viol_df['date_shift_five']\n",
    "\n",
    "# converting to integer\n",
    "viol_df['days_since_last_viol'] = viol_df['days_since_last_viol'].dt.days\n",
    "viol_df['days_since_last_viol_shift'] =viol_df['days_since_last_viol_shift'].dt.days\n",
    "viol_df['days_since_last_viol_shift_two'] = viol_df['days_since_last_viol_shift_two'].dt.days\n",
    "viol_df['days_since_last_viol_shift_three'] = viol_df['days_since_last_viol_shift_three'].dt.days\n",
    "viol_df['days_since_last_viol_shift_four'] = viol_df['days_since_last_viol_shift_four'].dt.days\n",
    "\n",
    "# replacing null values with zero\n",
    "viol_df['days_since_last_viol'] = viol_df['days_since_last_viol'].replace(np.nan, 0)\n",
    "viol_df['days_since_last_viol_shift'] = viol_df['days_since_last_viol_shift'].replace(np.nan, 0)\n",
    "viol_df['days_since_last_viol_shift_two'] = viol_df['days_since_last_viol_shift_two'].replace(np.nan, 0)\n",
    "viol_df['days_since_last_viol_shift_three'] = viol_df['days_since_last_viol_shift_three'].replace(np.nan, 0)\n",
    "viol_df['days_since_last_viol_shift_four'] = viol_df['days_since_last_viol_shift_four'].replace(np.nan, 0)\n",
    "\n",
    "\n",
    "# B) creating number of viol for 3 most recent violations ocurred  \n",
    "auxiliary_viol_df=  viol_df.groupby(['block_lot', 'date']).size().reset_index(name='N_violations_current_insp')\n",
    "auxiliary_viol_df['N_violations_last_insp'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_current_insp'].shift(+1)\n",
    "auxiliary_viol_df['N_violations_last_insp_shift'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_current_insp'].shift(+2)\n",
    "auxiliary_viol_df['N_violations_last_insp_shift_two'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_current_insp'].shift(+3)\n",
    "auxiliary_viol_df['N_violations_last_insp_shift_three'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_current_insp'].shift(+4)\n",
    "auxiliary_viol_df['N_violations_last_insp_shift_four'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_current_insp'].shift(+5)\n",
    "\n",
    "viol_df = viol_df.merge(auxiliary_viol_df, on = ['block_lot', 'date'], how = 'left')\n",
    "\n",
    "# replacing null values with zero\n",
    "viol_df['N_violations_current_insp'] = viol_df['N_violations_current_insp'].replace(np.nan, 0)\n",
    "viol_df['N_violations_last_insp'] = viol_df['N_violations_last_insp'].replace(np.nan, 0)\n",
    "viol_df['N_violations_last_insp_shift'] = viol_df['N_violations_last_insp_shift'].replace(np.nan, 0)\n",
    "viol_df['N_violations_last_insp_shift_two'] = viol_df['N_violations_last_insp_shift_two'].replace(np.nan, 0)\n",
    "viol_df['N_violations_last_insp_shift_three'] = viol_df['N_violations_last_insp_shift_three'].replace(np.nan, 0)\n",
    "viol_df['N_violations_last_insp_shift_four'] = viol_df['N_violations_last_insp_shift_four'].replace(np.nan, 0)\n",
    "\n",
    "\n",
    "# C) creating cummulative violations till date for 3 most recent violations ocurred \n",
    "auxiliary_viol_df['violations_cumsum_till_date'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_current_insp'].cumsum()\n",
    "auxiliary_viol_df['violations_cumsum_till_date_shift'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_last_insp'].cumsum()\n",
    "auxiliary_viol_df['violations_cumsum_till_date_shift_two'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_last_insp_shift'].cumsum()\n",
    "auxiliary_viol_df['violations_cumsum_till_date_shift_three'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_last_insp_shift_two'].cumsum()\n",
    "auxiliary_viol_df['violations_cumsum_till_date_shift_four'] = auxiliary_viol_df.groupby(['block_lot'])['N_violations_last_insp_shift_three'].cumsum()\n",
    "\n",
    "# dropping duplicate columns\n",
    "auxiliary_viol_df = auxiliary_viol_df.drop([\n",
    "    'N_violations_current_insp',\n",
    "    'N_violations_last_insp',\n",
    "    'N_violations_last_insp_shift',\n",
    "    'N_violations_last_insp_shift_two',\n",
    "    'N_violations_last_insp_shift_three',\n",
    "    'N_violations_last_insp_shift_four'],\n",
    "    axis =1)\n",
    "viol_df = viol_df.merge(auxiliary_viol_df, on = ['block_lot', 'date'], how = 'left')\n",
    "\n",
    "# replacing null values with zero\n",
    "viol_df['violations_cumsum_till_date'] = viol_df['violations_cumsum_till_date'].replace(np.nan, 0)\n",
    "viol_df['violations_cumsum_till_date_shift'] = viol_df['violations_cumsum_till_date_shift'].replace(np.nan, 0)\n",
    "viol_df['violations_cumsum_till_date_shift_two'] = viol_df['violations_cumsum_till_date_shift_two'].replace(np.nan, 0)\n",
    "viol_df['violations_cumsum_till_date_shift_three'] = viol_df['violations_cumsum_till_date_shift_three'].replace(np.nan, 0)\n",
    "viol_df['violations_cumsum_till_date_shift_four'] = viol_df['violations_cumsum_till_date_shift_four'].replace(np.nan, 0)\n",
    "\n",
    "\n",
    "\n",
    "#################################### TIER 2 DERIVATED VARIABLES ####################################\n",
    "# helper lists\n",
    "days_since_cols = ['days_since_last_viol','days_since_last_viol_shift','days_since_last_viol_shift_two',\n",
    "                      'days_since_last_viol_shift_three','days_since_last_viol_shift_four']\n",
    "\n",
    "N_violations_recent_insp_cols = ['N_violations_current_insp','N_violations_last_insp','N_violations_last_insp_shift',\n",
    "                                 'N_violations_last_insp_shift_two','N_violations_last_insp_shift_three',\n",
    "                                 'N_violations_last_insp_shift_four']\n",
    "\n",
    "# creating overal averages and stdevs\n",
    "viol_df['avg_days_between_previous_violations'] = viol_df[days_since_cols].mean(axis = 1)\n",
    "viol_df['avg_N_violations_recent_inspections'] = viol_df[N_violations_recent_insp_cols].mean(axis = 1)\n",
    "viol_df['StDev_days_between_previous_violations'] = viol_df[days_since_cols].std(axis = 1)\n",
    "viol_df['StDev_days_between_previous_violations'] = viol_df[days_since_cols].std(axis = 1)\n",
    "viol_df['ratio_avg_days_avg_N_violations'] = viol_df['avg_days_between_previous_violations']/viol_df['avg_N_violations_recent_inspections']\n",
    "\n",
    "# creating individual vs rest ratios\n",
    "for col in days_since_cols:\n",
    "    viol_df[\"{}_ratio_other\".format(col)] = viol_df[col] / viol_df[[x for x in days_since_cols if x!= col]].mean(axis=1)\n",
    "    viol_df[\"{}_ratio_other\".format(col)]= viol_df[\"{}_ratio_other\".format(col)].replace(np.nan, 0)\n",
    "\n",
    "for col in N_violations_recent_insp_cols:\n",
    "    viol_df[\"{}_ratio_other\".format(col)] = viol_df[col] / viol_df[[x for x in N_violations_recent_insp_cols if x!= col]].mean(axis=1)\n",
    "    viol_df[\"{}_ratio_other\".format(col)] = viol_df[\"{}_ratio_other\".format(col)] .replace(np.nan, 0)\n",
    "\n",
    "# adding columnwise subtractions\n",
    "auxiliary_viol_df = viol_df[days_since_cols]\n",
    "auxiliary_viol_df = auxiliary_viol_df.shift(-1, axis = 1) - auxiliary_viol_df \n",
    "auxiliary_viol_df = auxiliary_viol_df.iloc[:, :-1]\n",
    "auxiliary_viol_df.columns = [\"{}_subtracted\".format(x) for x in auxiliary_viol_df.columns]\n",
    "auxiliary_viol_df['AVG_days_since_columnwise_diff'] = auxiliary_viol_df.mean(axis = 1)\n",
    "auxiliary_viol_df['StDev_days_since_columnwise_diff'] = auxiliary_viol_df.std(axis = 1)\n",
    "viol_df = pd.concat([viol_df, auxiliary_viol_df], axis =1)\n",
    "\n",
    "auxiliary_viol_df = viol_df[N_violations_recent_insp_cols]\n",
    "auxiliary_viol_df = auxiliary_viol_df.iloc[:, :-1]\n",
    "auxiliary_viol_df = auxiliary_viol_df.shift(-1, axis = 1) - auxiliary_viol_df \n",
    "auxiliary_viol_df.columns = [\"{}_subtracted\".format(x) for x in auxiliary_viol_df.columns]\n",
    "auxiliary_viol_df['AVG_N_violations_columnwise_diff'] = auxiliary_viol_df.mean(axis = 1)\n",
    "auxiliary_viol_df['StDev_N_violations_columnwise_diff'] = auxiliary_viol_df.std(axis = 1)\n",
    "viol_df = pd.concat([viol_df, auxiliary_viol_df], axis =1)\n",
    "\n",
    "auxiliary_viol_df['AVG_days_since_N_violations_columnwise_diff'] =viol_df['AVG_days_since_columnwise_diff']\\\n",
    "                                                            /viol_df['AVG_N_violations_columnwise_diff']\n",
    "\n",
    "# adding columnwise ratios\n",
    "auxiliary_viol_df = viol_df[days_since_cols]\n",
    "auxiliary_viol_df = auxiliary_viol_df.shift(-1, axis = 1)/auxiliary_viol_df \n",
    "auxiliary_viol_df = auxiliary_viol_df.iloc[:, :-1]\n",
    "auxiliary_viol_df.columns = [\"{}_ratio_previous\".format(x) for x in auxiliary_viol_df.columns]\n",
    "viol_df[\"{}_ratio_other\".format(col)] = viol_df[\"{}_ratio_other\".format(col)] .replace(np.nan, 0)\n",
    "\n",
    "auxiliary_viol_df['AVG_days_since_columnwise_ratio'] = auxiliary_viol_df.mean(axis = 1)\n",
    "auxiliary_viol_df['StDev_days_since_columnwise_ratio'] = auxiliary_viol_df.std(axis = 1)\n",
    "viol_df = pd.concat([viol_df, auxiliary_viol_df], axis =1)\n",
    "\n",
    "auxiliary_viol_df = viol_df[N_violations_recent_insp_cols]\n",
    "auxiliary_viol_df = auxiliary_viol_df.shift(-1, axis = 1)/auxiliary_viol_df \n",
    "auxiliary_viol_df = auxiliary_viol_df.iloc[:, :-1]\n",
    "auxiliary_viol_df.columns = [\"{}_ratio_previous\".format(x) for x in auxiliary_viol_df.columns]\n",
    "auxiliary_viol_df['AVG_N_violations_columnwise_ratio'] = auxiliary_viol_df.mean(axis = 1)\n",
    "auxiliary_viol_df['StDev_N_violations_columnwise_ratio'] = auxiliary_viol_df.std(axis = 1)\n",
    "viol_df = pd.concat([viol_df, auxiliary_viol_df], axis =1)\n",
    "\n",
    "cum_sum_cols = ['violations_cumsum_till_date', 'violations_cumsum_till_date_shift','violations_cumsum_till_date_shift_two',\n",
    "                'violations_cumsum_till_date_shift_three','violations_cumsum_till_date_shift_four']\n",
    "auxiliary_viol_df = viol_df[cum_sum_cols]\n",
    "auxiliary_viol_df = auxiliary_viol_df/auxiliary_viol_df.shift(-1, axis = 1) \n",
    "auxiliary_viol_df = auxiliary_viol_df.iloc[:, :-1]\n",
    "auxiliary_viol_df.columns = [\"{}_ratio_previous\".format(x) for x in auxiliary_viol_df.columns]\n",
    "auxiliary_viol_df['AVG_cumsum_columnwise_ratio'] = auxiliary_viol_df.mean(axis = 1)\n",
    "auxiliary_viol_df['StDev_cumsum_columnwise_ratio'] = auxiliary_viol_df.std(axis = 1)\n",
    "viol_df = pd.concat([viol_df, auxiliary_viol_df], axis =1)\n",
    "\n",
    "# adding inter group columnwise ratios\n",
    "viol_df['ratio_days_since_N_violations_last_insp'] = viol_df['days_since_last_viol']/viol_df['N_violations_last_insp']\n",
    "viol_df['ratio_days_since_N_violations_last_insp_shift'] = viol_df['days_since_last_viol_shift']/viol_df['N_violations_last_insp_shift']\n",
    "viol_df['ratio_days_since_N_violations_last_insp_shift_two'] = viol_df['days_since_last_viol_shift_two']/viol_df['N_violations_last_insp_shift_two']\n",
    "viol_df['ratio_days_since_N_violations_last_insp_shift_three'] = viol_df['days_since_last_viol_shift_three']/viol_df['N_violations_last_insp_shift_three']\n",
    "viol_df['ratio_days_since_N_violations_last_insp_shift_four'] = viol_df['days_since_last_viol_shift_four']/viol_df['N_violations_last_insp_shift_four']\n",
    "\n",
    "\n",
    "viol_df['AVG_columnwise_ratio_days_since_N_violations'] = viol_df[['ratio_days_since_N_violations_last_insp',\n",
    "                                                          'ratio_days_since_N_violations_last_insp_shift_two',\n",
    "                                                          'ratio_days_since_N_violations_last_insp_shift_three',\n",
    "                                                          'ratio_days_since_N_violations_last_insp_shift_four']].mean(axis = 1)\n",
    "\n",
    "viol_df['StDev_columnwise_ratio_days_since_N_violations'] = viol_df[['ratio_days_since_N_violations_last_insp',\n",
    "                                                          'ratio_days_since_N_violations_last_insp_shift_two',\n",
    "                                                          'ratio_days_since_N_violations_last_insp_shift_three',\n",
    "                                                          'ratio_days_since_N_violations_last_insp_shift_four']].std(axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### individual category level feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building section\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other section\n",
      "fire section\n",
      "sanitation section\n",
      "interior surfaces section\n",
      "plumbing and electrical section\n",
      "security requirements section\n",
      "smoke detection section\n",
      "lead section\n",
      "hco\n",
      "building section\n",
      "other section\n",
      "fire section\n",
      "sanitation section\n",
      "interior surfaces section\n",
      "plumbing and electrical section\n",
      "security requirements section\n",
      "smoke detection section\n",
      "lead section\n",
      "hco\n"
     ]
    }
   ],
   "source": [
    "#################################### TIER I DERIVATED VARIABLES ####################################\n",
    "\n",
    "# A) calculating days since last violations for 3 most recent violations ocurred \n",
    "for i in viol_df['NOV Category Description'].unique():\n",
    "    print(i)\n",
    "    \n",
    "    auxiliary_viol_df = viol_df[['block_lot', 'date','NOV Category Description']]\n",
    "    auxiliary_viol_df['{}_date'.format(i)]= np.where(auxiliary_viol_df['NOV Category Description'] == i, 1, np.nan)\n",
    "    \n",
    "    auxiliary_viol_df = auxiliary_viol_df.dropna(subset= ['{}_date'.format(i)])\n",
    "    auxiliary_viol_df['{}_date'.format(i)] = auxiliary_viol_df['date']\n",
    "    auxiliary_viol_df = auxiliary_viol_df.drop_duplicates(subset = ['block_lot', 'date'])\n",
    "    auxiliary_viol_df = auxiliary_viol_df.drop(['NOV Category Description'], axis =1)\n",
    "\n",
    "    auxiliary_viol_df['{}_date_shift'.format(i)] = auxiliary_viol_df.groupby(['block_lot'])['{}_date'.format(i)].shift(+1)\n",
    "    auxiliary_viol_df['{}_date_shift_two'.format(i)] = auxiliary_viol_df.groupby(['block_lot'])['{}_date'.format(i)].shift(+2)\n",
    "    auxiliary_viol_df['{}_date_shift_three'.format(i)] = auxiliary_viol_df.groupby(['block_lot'])['{}_date'.format(i)].shift(+3)\n",
    "    auxiliary_viol_df['{}_date_shift_four'.format(i)] = auxiliary_viol_df.groupby(['block_lot'])['{}_date'.format(i)].shift(+4)\n",
    "\n",
    "    viol_df = viol_df.merge(auxiliary_viol_df, on = ['block_lot', 'date'], how = 'left')\n",
    "    viol_df['{}_date_ffill'.format(i)] = viol_df.groupby(['block_lot'])['{}_date'.format(i)].ffill()\n",
    "    viol_df['{}_date_ffill_shift'.format(i)] = viol_df.groupby(['block_lot'])['{}_date_shift'.format(i)].ffill()\n",
    "    viol_df['{}_ffill_shift_two'.format(i)] = viol_df.groupby(['block_lot'])['{}_date_shift_two'.format(i)].ffill()\n",
    "    viol_df['{}_ffill_shift_three'.format(i)] = viol_df.groupby(['block_lot'])['{}_date_shift_three'.format(i)].ffill()\n",
    "    viol_df['{}_ffill_shift_four'.format(i)] = viol_df.groupby(['block_lot'])['{}_date_shift_four'.format(i)].ffill()\n",
    "\n",
    "    viol_df['days_since_{}'.format(i)] = viol_df['date'] - viol_df['{}_date_ffill'.format(i)]\n",
    "    viol_df['days_since_{}_shift'.format(i)] = viol_df['date'] - viol_df['{}_date_ffill_shift'.format(i)]\n",
    "    viol_df['days_since_{}_shift_two'.format(i)] = viol_df['date'] - viol_df['{}_ffill_shift_two'.format(i)]\n",
    "    viol_df['days_since_{}_shift_three'.format(i)] = viol_df['date'] - viol_df['{}_ffill_shift_three'.format(i)]\n",
    "    viol_df['days_since_{}_shift_four'.format(i)] = viol_df['date'] - viol_df['{}_ffill_shift_four'.format(i)]\n",
    "    \n",
    "    viol_df['days_since_{}'.format(i)] =  viol_df['days_since_{}'.format(i)].dt.days\n",
    "    viol_df['days_since_{}_shift'.format(i)] =  viol_df['days_since_{}_shift'.format(i)].dt.days\n",
    "    viol_df['days_since_{}_shift_two'.format(i)] =  viol_df['days_since_{}_shift_two'.format(i)].dt.days\n",
    "    viol_df['days_since_{}_shift_three'.format(i)] =  viol_df['days_since_{}_shift_three'.format(i)].dt.days\n",
    "    viol_df['days_since_{}_shift_four'.format(i)] = viol_df['days_since_{}_shift_four'.format(i)].dt.days   \n",
    "    \n",
    "\n",
    "    viol_df['days_since_{}'.format(i)] =  viol_df['days_since_{}'.format(i)].replace(np.nan, 0)\n",
    "    viol_df['days_since_{}_shift'.format(i)] =  viol_df['days_since_{}_shift'.format(i)].replace(np.nan, 0)\n",
    "    viol_df['days_since_{}_shift_two'.format(i)] =  viol_df['days_since_{}_shift_two'.format(i)].replace(np.nan, 0)\n",
    "    viol_df['days_since_{}_shift_three'.format(i)] =  viol_df['days_since_{}_shift_three'.format(i)].replace(np.nan, 0)\n",
    "    viol_df['days_since_{}_shift_four'.format(i)] = viol_df['days_since_{}_shift_four'.format(i)].replace(np.nan, 0)\n",
    "\n",
    "    viol_df = viol_df.drop([\n",
    "        '{}_date'.format(i),\n",
    "        '{}_date_shift'.format(i),\n",
    "        '{}_date_shift_two'.format(i),\n",
    "        '{}_date_shift_three'.format(i),\n",
    "        '{}_date_shift_four'.format(i),\n",
    "        '{}_date_ffill'.format(i),\n",
    "        '{}_date_ffill_shift'.format(i),\n",
    "        '{}_ffill_shift_two'.format(i),\n",
    "        '{}_ffill_shift_three'.format(i),\n",
    "        '{}_ffill_shift_four'.format(i)],\n",
    "        axis = 1)\n",
    "\n",
    "\n",
    "# B) # C) creating number of viol and cummulative violations till date for 5 most recent violations ocurred  \n",
    "viol_df['dummy'] = 1\n",
    "pivot_viol_df = pd.pivot_table(viol_df, index=['block_lot','date',], columns=['NOV Category Description'], values=[\"dummy\"], aggfunc=np.sum)\n",
    "pivot_viol_df = pivot_viol_df.reset_index(drop = False)\n",
    "pivot_viol_df.columns = pivot_viol_df.columns.droplevel(0)\n",
    "pivot_viol_df.columns.values[[0, 1]] = ['block_lot', 'date']\n",
    "\n",
    "for i in viol_df['NOV Category Description'].unique():\n",
    "    print(i)\n",
    "    pivot_viol_df['N_{}_violations_current'.format(i)] = pivot_viol_df[i]\n",
    "    pivot_viol_df['N_{}_violations_previous'.format(i)] = pivot_viol_df.groupby(['block_lot'])[i].shift(+1)\n",
    "    pivot_viol_df['N_{}_violations_previous_shift_one'.format(i)] = pivot_viol_df.groupby(['block_lot'])[i].shift(+2)\n",
    "    pivot_viol_df['N_{}_violations_previous_shift_two'.format(i)] = pivot_viol_df.groupby(['block_lot'])[i].shift(+3)\n",
    "    pivot_viol_df['N_{}_violations_previous_shift_three'.format(i)] = pivot_viol_df.groupby(['block_lot'])[i].shift(+4)\n",
    "    pivot_viol_df['N_{}_violations_previous_shift_four'.format(i)] = pivot_viol_df.groupby(['block_lot'])[i].shift(+5)\n",
    "\n",
    "    pivot_viol_df['{}_cumsum_till_date'.format(i)] = pivot_viol_df.groupby(['block_lot'])['N_{}_violations_current'.format(i)].cumsum()\n",
    "    pivot_viol_df['{}_cumsum_till_date_shift'.format(i)] = pivot_viol_df.groupby(['block_lot'])['N_{}_violations_previous'.format(i)].cumsum()\n",
    "    pivot_viol_df['{}_cumsum_till_date_shift_two'.format(i)] = pivot_viol_df.groupby(['block_lot'])['N_{}_violations_previous_shift_one'.format(i)].cumsum()\n",
    "    pivot_viol_df['{}_cumsum_till_date_shift_three'.format(i)] = pivot_viol_df.groupby(['block_lot'])['N_{}_violations_previous_shift_two'.format(i)].cumsum()\n",
    "    pivot_viol_df['{}_cumsum_till_date_shift_four'.format(i)] = pivot_viol_df.groupby(['block_lot'])['N_{}_violations_previous_shift_three'.format(i)].cumsum()\n",
    "    \n",
    "    pivot_viol_df['{}_cumsum_till_date'.format(i)] = pivot_viol_df.groupby(['block_lot'])['{}_cumsum_till_date'.format(i)].ffill()\n",
    "    pivot_viol_df['{}_cumsum_till_date_shift'.format(i)] = pivot_viol_df.groupby(['block_lot'])['{}_cumsum_till_date_shift'.format(i)].ffill()\n",
    "    pivot_viol_df['{}_cumsum_till_date_shift_two'.format(i)] = pivot_viol_df.groupby(['block_lot'])['{}_cumsum_till_date_shift_two'.format(i)].ffill()\n",
    "    pivot_viol_df['{}_cumsum_till_date_shift_three'.format(i)] = pivot_viol_df.groupby(['block_lot'])['{}_cumsum_till_date_shift_three'.format(i)].ffill()\n",
    "    pivot_viol_df['{}_cumsum_till_date_shift_four'.format(i)] = pivot_viol_df.groupby(['block_lot'])['{}_cumsum_till_date_shift_four'.format(i)].ffill()\n",
    "    \n",
    "\n",
    "    viol_df =  pd.DataFrame.merge(viol_df,\n",
    "                          pivot_viol_df[[\n",
    "                              'block_lot', 'date',\n",
    "                              'N_{}_violations_current'.format(i),\n",
    "\n",
    "                              'N_{}_violations_previous'.format(i),\n",
    "                              'N_{}_violations_previous_shift_one'.format(i),\n",
    "                              'N_{}_violations_previous_shift_two'.format(i),\n",
    "                              'N_{}_violations_previous_shift_three'.format(i),\n",
    "                              'N_{}_violations_previous_shift_four'.format(i),\n",
    "                            '{}_cumsum_till_date'.format(i),\n",
    "                            '{}_cumsum_till_date_shift'.format(i),\n",
    "                            '{}_cumsum_till_date_shift_two'.format(i),\n",
    "                            '{}_cumsum_till_date_shift_three'.format(i),\n",
    "                            '{}_cumsum_till_date_shift_four'.format(i)]],\n",
    "                          on=['block_lot', 'date'],\n",
    "                          how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modelling pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining the target variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating target variable for MODELING SCENARIO A: modelling the possibility of any violation happening in the next 60 days \n",
    "\n",
    "auxiliary_df = viol_df[['block_lot', 'date']].drop_duplicates()\n",
    "auxiliary_df['date_shift'] = auxiliary_df.groupby(['block_lot'])['date'].shift(-1)\n",
    "auxiliary_df['days_difference'] = auxiliary_df['date_shift'] - auxiliary_df['date']  \n",
    "auxiliary_df['days_difference'] = auxiliary_df['days_difference'].dt.days\n",
    "# replacing nans with zeroes. NaN values appear in block_lot -date scenarios where there is no info about the next vilation at the blocklot\n",
    "auxiliary_df['days_difference'] = np.where(auxiliary_df['days_difference'].isna(), 0, auxiliary_df['days_difference'])\n",
    "# creating the target variable for classification\n",
    "auxiliary_df['violation_in_following_120_days'] = np.where(auxiliary_df['days_difference'] <= 120, 1,0)\n",
    "# # removing from modelling most recent scenarios (small number of cases)- for these we are not sure if the violation will happen or not in the following period because this perios is still happening\n",
    "# auxiliary_df = auxiliary_df[auxiliary_df['date'] <= auxiliary_df['date'].max() - timedelta(days=60)]\n",
    "viol_df = viol_df.merge(auxiliary_df[['violation_in_following_120_days', 'block_lot', 'date']], on = ['block_lot', 'date'], how = 'left')\n",
    "\n",
    "\n",
    "# creating target variable for MODELING SCENARIO B: modelling the expected ammount of any type of violations during the next violation-ocurring inspection\n",
    "auxiliary_df=  viol_df.groupby(['block_lot', 'date']).size().reset_index(name='counts')\n",
    "auxiliary_df['N_violations_next_insp'] = auxiliary_df.groupby(['block_lot'])['counts'].shift(-1)\n",
    "# replacing nans with zeroes. NaN values appear in block_lot -date scenarios where there is no info about the next vilation at the blocklot\n",
    "auxiliary_df['N_violations_next_insp'] = np.where(auxiliary_df['N_violations_next_insp'].isna(), 0,\\\n",
    "                                                  auxiliary_df['N_violations_next_insp'])\n",
    "# removing from modelling most recent scenarios (small number of cases)- for these we are not sure if the violation will happen or not in the following period because this perios is still happening\n",
    "auxiliary_df = auxiliary_df[auxiliary_df['date'] <= auxiliary_df['date'].max() - timedelta(days=60)]\n",
    "auxiliary_df[auxiliary_df['block_lot'] == '3547_018B']\n",
    "viol_df = viol_df.merge(auxiliary_df[['N_violations_next_insp', 'block_lot', 'date']], on = ['block_lot', 'date'], how = 'left')\n",
    "\n",
    "# Standardizing the dataframe after all of the feature engineering so far has been done\n",
    "viol_df = viol_df.drop_duplicates(subset = ['block_lot', 'date'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test validation rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing the dataset into scoring, testing, validation and training\n",
    "# time series adjusted k fold cross validation (https://stats.stackexchange.com/a/366288) was not feasible, since we dont have a traditional\n",
    "# scoring data will be the most recent case of each block lot i violations dataset, plus all blocklots which werent present at this dataset\n",
    "\n",
    "\n",
    "# temporarily reverse date order\n",
    "# add the groupby row number\n",
    "viol_df = viol_df.sort_values(by='date', ascending = False)\n",
    "viol_df['ordinality'] = viol_df.groupby(['block_lot']).cumcount()+1\n",
    "viol_df['ordinality']= viol_df['ordinality'][::-1]\n",
    "viol_df[viol_df['block_lot']== '4710_003'].head(20)\n",
    "\n",
    "# returning back the normal ascending sort\n",
    "viol_df = viol_df.sort_values(by='date')\n",
    "\n",
    "# data split rules for first round of model building\n",
    "viol_df['train_test_validation_split'] = np.where(viol_df['ordinality'] == 1, 'score_data', np.nan)\n",
    "viol_df['train_test_validation_split'] = np.where(viol_df['ordinality'] == 2, 'test_data',viol_df['train_test_validation_split'])\n",
    "viol_df['train_test_validation_split'] = np.where(viol_df['ordinality'] == 3, 'validation_data', viol_df['train_test_validation_split'])\n",
    "viol_df['train_test_validation_split'] = np.where(viol_df['ordinality'] > 3, 'train_data', viol_df['train_test_validation_split'] )\n",
    "\n",
    "# data split rules for second round of model building, when the best hyperparameters are known\n",
    "viol_df['train_score_split'] = np.where(viol_df['ordinality'] == 1, 'score_data', np.nan)\n",
    "viol_df['train_score_split'] = np.where(viol_df['ordinality'] != 1, 'train_data',viol_df['train_score_split'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viol_df (67155, 234)\n",
      "train_data first model (21367, 234)\n",
      "train_data_second_model (28907, 234)\n",
      "train_data_final_model (42455, 234)\n",
      "validation_data (7540, 234)\n",
      "test_data (13548, 234)\n"
     ]
    }
   ],
   "source": [
    "# dropping columns not required for modelling\n",
    "cols_to_drop = [\n",
    "    'Block',\n",
    "    'Lot',\n",
    "    'Street Number',\n",
    "    'Street Name',\n",
    "    'Status',\n",
    "    'NOV Category Description',\n",
    "    'Item',\n",
    "    'Neighborhoods - Analysis Boundaries',\n",
    "    'Supervisor District',\n",
    "    'Location',\n",
    "    'date_shift_one',\n",
    "    'date_shift_two',\n",
    "    'date_shift_three',\n",
    "    'date_shift_four',\n",
    "    'date_shift_five',\n",
    "    'dummy',\n",
    "    'block_lot',\n",
    "    'date',\n",
    "    'ordinality'\n",
    "]\n",
    "viol_df = viol_df.drop(cols_to_drop, axis = 1)\n",
    "\n",
    "# replacing Nans and inf values with zeros. These came up in feature engineering while dividing with zero, NaN and similar\n",
    "viol_df = viol_df.replace(np.nan, 0)\n",
    "viol_df = viol_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "# subsetting datasets for first round of modelling (for optimal model validation purposes)\n",
    "train_data = viol_df[viol_df['train_test_validation_split'] == 'train_data']\n",
    "validation_data = viol_df[viol_df['train_test_validation_split'] == 'validation_data']\n",
    "\n",
    "# subsetting dataset for second round of modelling (for testing purposes)\n",
    "train_data_second_model = viol_df[(viol_df['train_test_validation_split'] == 'train_data')\\\n",
    "                                  | (viol_df['train_test_validation_split'] == 'validation_data')]\n",
    "test_data = viol_df[viol_df['train_test_validation_split'] == 'test_data']\n",
    "\n",
    "\n",
    "# subsetting datasets for final model (for final scoring purposes)\n",
    "train_data_final_model = viol_df[viol_df['train_score_split'] == 'train_data']\n",
    "score_data_final_model = viol_df[viol_df['train_score_split'] == 'score_data']\n",
    "\n",
    "# seeing shapes \n",
    "print(\"viol_df\", viol_df.shape)\n",
    "print(\"train_data first model\",train_data.shape)\n",
    "print(\"train_data_second_model\",train_data_second_model.shape)\n",
    "print(\"train_data_final_model\",train_data_final_model.shape)\n",
    "print(\"validation_data\",validation_data.shape)\n",
    "print(\"test_data\",test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_data_final_model (24700, 234)\n"
     ]
    }
   ],
   "source": [
    "print(\"score_data_final_model\",score_data_final_model.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaling datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying columns for scaling \n",
    "numerical_predictor_features = [\n",
    "    'days_since_last_viol',\n",
    "    'days_since_last_viol_shift',\n",
    "    'days_since_last_viol_shift_two',\n",
    "    'days_since_last_viol_shift_three',\n",
    "    'days_since_last_viol_shift_four',\n",
    "    'N_violations_current_insp',\n",
    "    'N_violations_last_insp',\n",
    "    'N_violations_last_insp_shift',\n",
    "    'N_violations_last_insp_shift_two',\n",
    "    'N_violations_last_insp_shift_three',\n",
    "    'N_violations_last_insp_shift_four',\n",
    "    'violations_cumsum_till_date',\n",
    "    'violations_cumsum_till_date_shift',\n",
    "    'violations_cumsum_till_date_shift_two',\n",
    "    'violations_cumsum_till_date_shift_three',\n",
    "    'violations_cumsum_till_date_shift_four',\n",
    "    'avg_days_between_previous_violations',\n",
    "    'avg_N_violations_recent_inspections',\n",
    "    'StDev_days_between_previous_violations',\n",
    "    'ratio_avg_days_avg_N_violations',\n",
    "    'days_since_last_viol_ratio_other',\n",
    "    'days_since_last_viol_shift_ratio_other',\n",
    "    'days_since_last_viol_shift_two_ratio_other',\n",
    "    'days_since_last_viol_shift_three_ratio_other',\n",
    "    'days_since_last_viol_shift_four_ratio_other',\n",
    "    'N_violations_current_insp_ratio_other',\n",
    "    'N_violations_last_insp_ratio_other',\n",
    "    'N_violations_last_insp_shift_ratio_other',\n",
    "    'N_violations_last_insp_shift_two_ratio_other',\n",
    "    'N_violations_last_insp_shift_three_ratio_other',\n",
    "    'N_violations_last_insp_shift_four_ratio_other',\n",
    "    'days_since_last_viol_subtracted',\n",
    "    'days_since_last_viol_shift_subtracted',\n",
    "    'days_since_last_viol_shift_two_subtracted',\n",
    "    'days_since_last_viol_shift_three_subtracted',\n",
    "    'AVG_days_since_columnwise_diff',\n",
    "    'StDev_days_since_columnwise_diff',\n",
    "    'N_violations_current_insp_subtracted',\n",
    "    'N_violations_last_insp_subtracted',\n",
    "    'N_violations_last_insp_shift_subtracted',\n",
    "    'N_violations_last_insp_shift_two_subtracted',\n",
    "    'N_violations_last_insp_shift_three_subtracted',\n",
    "    'AVG_N_violations_columnwise_diff',\n",
    "    'StDev_N_violations_columnwise_diff',\n",
    "    'days_since_last_viol_ratio_previous',\n",
    "    'days_since_last_viol_shift_ratio_previous',\n",
    "    'days_since_last_viol_shift_two_ratio_previous',\n",
    "    'days_since_last_viol_shift_three_ratio_previous',\n",
    "    'AVG_days_since_columnwise_ratio',\n",
    "    'StDev_days_since_columnwise_ratio',\n",
    "    'N_violations_current_insp_ratio_previous',\n",
    "    'N_violations_last_insp_ratio_previous',\n",
    "    'N_violations_last_insp_shift_ratio_previous',\n",
    "    'N_violations_last_insp_shift_two_ratio_previous',\n",
    "    'N_violations_last_insp_shift_three_ratio_previous',\n",
    "    'AVG_N_violations_columnwise_ratio',\n",
    "    'StDev_N_violations_columnwise_ratio',\n",
    "    'violations_cumsum_till_date_ratio_previous',\n",
    "    'violations_cumsum_till_date_shift_ratio_previous',\n",
    "    'violations_cumsum_till_date_shift_two_ratio_previous',\n",
    "    'violations_cumsum_till_date_shift_three_ratio_previous',\n",
    "    'AVG_cumsum_columnwise_ratio',\n",
    "    'StDev_cumsum_columnwise_ratio',\n",
    "    'ratio_days_since_N_violations_last_insp',\n",
    "    'ratio_days_since_N_violations_last_insp_shift',\n",
    "    'ratio_days_since_N_violations_last_insp_shift_two',\n",
    "    'ratio_days_since_N_violations_last_insp_shift_three',\n",
    "    'ratio_days_since_N_violations_last_insp_shift_four',\n",
    "    'AVG_columnwise_ratio_days_since_N_violations',\n",
    "    'StDev_columnwise_ratio_days_since_N_violations',\n",
    "    'days_since_building section',\n",
    "    'days_since_building section_shift',\n",
    "    'days_since_building section_shift_two',\n",
    "    'days_since_building section_shift_three',\n",
    "    'days_since_building section_shift_four',\n",
    "    'days_since_other section',\n",
    "    'days_since_other section_shift',\n",
    "    'days_since_other section_shift_two',\n",
    "    'days_since_other section_shift_three',\n",
    "    'days_since_other section_shift_four',\n",
    "    'days_since_fire section',\n",
    "    'days_since_fire section_shift',\n",
    "    'days_since_fire section_shift_two',\n",
    "    'days_since_fire section_shift_three',\n",
    "    'days_since_fire section_shift_four',\n",
    "    'days_since_sanitation section',\n",
    "    'days_since_sanitation section_shift',\n",
    "    'days_since_sanitation section_shift_two',\n",
    "    'days_since_sanitation section_shift_three',\n",
    "    'days_since_sanitation section_shift_four',\n",
    "    'days_since_interior surfaces section',\n",
    "    'days_since_interior surfaces section_shift',\n",
    "    'days_since_interior surfaces section_shift_two',\n",
    "    'days_since_interior surfaces section_shift_three',\n",
    "    'days_since_interior surfaces section_shift_four',\n",
    "    'days_since_plumbing and electrical section',\n",
    "    'days_since_plumbing and electrical section_shift',\n",
    "    'days_since_plumbing and electrical section_shift_two',\n",
    "    'days_since_plumbing and electrical section_shift_three',\n",
    "    'days_since_plumbing and electrical section_shift_four',\n",
    "    'days_since_security requirements section',\n",
    "    'days_since_security requirements section_shift',\n",
    "    'days_since_security requirements section_shift_two',\n",
    "    'days_since_security requirements section_shift_three',\n",
    "    'days_since_security requirements section_shift_four',\n",
    "    'days_since_smoke detection section',\n",
    "    'days_since_smoke detection section_shift',\n",
    "    'days_since_smoke detection section_shift_two',\n",
    "    'days_since_smoke detection section_shift_three',\n",
    "    'days_since_smoke detection section_shift_four',\n",
    "    'days_since_lead section',\n",
    "    'days_since_lead section_shift',\n",
    "    'days_since_lead section_shift_two',\n",
    "    'days_since_lead section_shift_three',\n",
    "    'days_since_lead section_shift_four',\n",
    "    'days_since_hco',\n",
    "    'days_since_hco_shift',\n",
    "    'days_since_hco_shift_two',\n",
    "    'days_since_hco_shift_three',\n",
    "    'days_since_hco_shift_four',\n",
    "    'N_building section_violations_current',\n",
    "    'N_building section_violations_previous',\n",
    "    'N_building section_violations_previous_shift_one',\n",
    "    'N_building section_violations_previous_shift_two',\n",
    "    'N_building section_violations_previous_shift_three',\n",
    "    'N_building section_violations_previous_shift_four',\n",
    "    'building section_cumsum_till_date',\n",
    "    'building section_cumsum_till_date_shift',\n",
    "    'building section_cumsum_till_date_shift_two',\n",
    "    'building section_cumsum_till_date_shift_three',\n",
    "    'building section_cumsum_till_date_shift_four',\n",
    "    'N_other section_violations_current',\n",
    "    'N_other section_violations_previous',\n",
    "    'N_other section_violations_previous_shift_one',\n",
    "    'N_other section_violations_previous_shift_two',\n",
    "    'N_other section_violations_previous_shift_three',\n",
    "    'N_other section_violations_previous_shift_four',\n",
    "    'other section_cumsum_till_date',\n",
    "    'other section_cumsum_till_date_shift',\n",
    "    'other section_cumsum_till_date_shift_two',\n",
    "    'other section_cumsum_till_date_shift_three',\n",
    "    'other section_cumsum_till_date_shift_four',\n",
    "    'N_fire section_violations_current',\n",
    "    'N_fire section_violations_previous',\n",
    "    'N_fire section_violations_previous_shift_one',\n",
    "    'N_fire section_violations_previous_shift_two',\n",
    "    'N_fire section_violations_previous_shift_three',\n",
    "    'N_fire section_violations_previous_shift_four',\n",
    "    'fire section_cumsum_till_date',\n",
    "    'fire section_cumsum_till_date_shift',\n",
    "    'fire section_cumsum_till_date_shift_two',\n",
    "    'fire section_cumsum_till_date_shift_three',\n",
    "    'fire section_cumsum_till_date_shift_four',\n",
    "    'N_sanitation section_violations_current',\n",
    "    'N_sanitation section_violations_previous',\n",
    "    'N_sanitation section_violations_previous_shift_one',\n",
    "    'N_sanitation section_violations_previous_shift_two',\n",
    "    'N_sanitation section_violations_previous_shift_three',\n",
    "    'N_sanitation section_violations_previous_shift_four',\n",
    "    'sanitation section_cumsum_till_date',\n",
    "    'sanitation section_cumsum_till_date_shift',\n",
    "    'sanitation section_cumsum_till_date_shift_two',\n",
    "    'sanitation section_cumsum_till_date_shift_three',\n",
    "    'sanitation section_cumsum_till_date_shift_four',\n",
    "    'N_interior surfaces section_violations_current',\n",
    "    'N_interior surfaces section_violations_previous',\n",
    "    'N_interior surfaces section_violations_previous_shift_one',\n",
    "    'N_interior surfaces section_violations_previous_shift_two',\n",
    "    'N_interior surfaces section_violations_previous_shift_three',\n",
    "    'N_interior surfaces section_violations_previous_shift_four',\n",
    "    'interior surfaces section_cumsum_till_date',\n",
    "    'interior surfaces section_cumsum_till_date_shift',\n",
    "    'interior surfaces section_cumsum_till_date_shift_two',\n",
    "    'interior surfaces section_cumsum_till_date_shift_three',\n",
    "    'interior surfaces section_cumsum_till_date_shift_four',\n",
    "    'N_plumbing and electrical section_violations_current',\n",
    "    'N_plumbing and electrical section_violations_previous',\n",
    "    'N_plumbing and electrical section_violations_previous_shift_one',\n",
    "    'N_plumbing and electrical section_violations_previous_shift_two',\n",
    "    'N_plumbing and electrical section_violations_previous_shift_three',\n",
    "    'N_plumbing and electrical section_violations_previous_shift_four',\n",
    "    'plumbing and electrical section_cumsum_till_date',\n",
    "    'plumbing and electrical section_cumsum_till_date_shift',\n",
    "    'plumbing and electrical section_cumsum_till_date_shift_two',\n",
    "    'plumbing and electrical section_cumsum_till_date_shift_three',\n",
    "    'plumbing and electrical section_cumsum_till_date_shift_four',\n",
    "    'N_security requirements section_violations_current',\n",
    "    'N_security requirements section_violations_previous',\n",
    "    'N_security requirements section_violations_previous_shift_one',\n",
    "    'N_security requirements section_violations_previous_shift_two',\n",
    "    'N_security requirements section_violations_previous_shift_three',\n",
    "    'N_security requirements section_violations_previous_shift_four',\n",
    "    'security requirements section_cumsum_till_date',\n",
    "    'security requirements section_cumsum_till_date_shift',\n",
    "    'security requirements section_cumsum_till_date_shift_two',\n",
    "    'security requirements section_cumsum_till_date_shift_three',\n",
    "    'security requirements section_cumsum_till_date_shift_four',\n",
    "    'N_smoke detection section_violations_current',\n",
    "    'N_smoke detection section_violations_previous',\n",
    "    'N_smoke detection section_violations_previous_shift_one',\n",
    "    'N_smoke detection section_violations_previous_shift_two',\n",
    "    'N_smoke detection section_violations_previous_shift_three',\n",
    "    'N_smoke detection section_violations_previous_shift_four',\n",
    "    'smoke detection section_cumsum_till_date',\n",
    "    'smoke detection section_cumsum_till_date_shift',\n",
    "    'smoke detection section_cumsum_till_date_shift_two',\n",
    "    'smoke detection section_cumsum_till_date_shift_three',\n",
    "    'smoke detection section_cumsum_till_date_shift_four',\n",
    "    'N_lead section_violations_current',\n",
    "    'N_lead section_violations_previous',\n",
    "    'N_lead section_violations_previous_shift_one',\n",
    "    'N_lead section_violations_previous_shift_two',\n",
    "    'N_lead section_violations_previous_shift_three',\n",
    "    'N_lead section_violations_previous_shift_four',\n",
    "    'lead section_cumsum_till_date',\n",
    "    'lead section_cumsum_till_date_shift',\n",
    "    'lead section_cumsum_till_date_shift_two',\n",
    "    'lead section_cumsum_till_date_shift_three',\n",
    "    'lead section_cumsum_till_date_shift_four',\n",
    "    'N_hco_violations_current',\n",
    "    'N_hco_violations_previous',\n",
    "    'N_hco_violations_previous_shift_one',\n",
    "    'N_hco_violations_previous_shift_two',\n",
    "    'N_hco_violations_previous_shift_three',\n",
    "    'N_hco_violations_previous_shift_four',\n",
    "    'hco_cumsum_till_date',\n",
    "    'hco_cumsum_till_date_shift',\n",
    "    'hco_cumsum_till_date_shift_two',\n",
    "    'hco_cumsum_till_date_shift_three',\n",
    "    'hco_cumsum_till_date_shift_four']\n",
    "\n",
    "\n",
    "# SCALING DATA \n",
    "# first model\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train_data.loc[:, numerical_predictor_features])\n",
    "train_scaled_features = scaler.transform(train_data.loc[:, numerical_predictor_features])\n",
    "train_scaled_features = pd.DataFrame(train_scaled_features)\n",
    "train_scaled_features.columns = numerical_predictor_features\n",
    "train_data = train_data.drop(numerical_predictor_features, axis=1).reset_index(level=0, drop=True)\n",
    "train_data = pd.concat((train_data, train_scaled_features), axis=1)\n",
    "\n",
    "validation_scaled_features = scaler.transform(validation_data.loc[:, numerical_predictor_features])\n",
    "validation_scaled_features = pd.DataFrame(validation_scaled_features)\n",
    "validation_scaled_features.columns = numerical_predictor_features\n",
    "validation_data = validation_data.drop(numerical_predictor_features, axis=1).reset_index(level=0, drop=True)\n",
    "validation_data = pd.concat((validation_data, validation_scaled_features), axis=1)\n",
    "\n",
    "\n",
    "# second model\n",
    "scaler_second_model = StandardScaler()\n",
    "scaler_second_model = scaler_second_model.fit(train_data_second_model.loc[:, numerical_predictor_features])\n",
    "train_second_scaled_features = scaler_second_model.transform(train_data_second_model.loc[:, numerical_predictor_features])\n",
    "train_second_scaled_features = pd.DataFrame(train_second_scaled_features)\n",
    "train_second_scaled_features.columns = numerical_predictor_features\n",
    "train_data_second_model = train_data_second_model.drop(numerical_predictor_features, axis=1).reset_index(level=0, drop=True)\n",
    "train_data_second_model = pd.concat((train_data_second_model, train_second_scaled_features), axis=1)\n",
    "\n",
    "test_scaled_features = scaler_second_model.transform(test_data.loc[:, numerical_predictor_features])\n",
    "test_scaled_features = pd.DataFrame(test_scaled_features)\n",
    "test_scaled_features.columns = numerical_predictor_features\n",
    "test_data = test_data.drop(numerical_predictor_features, axis=1).reset_index(level=0, drop=True)\n",
    "test_data = pd.concat((test_data, test_scaled_features), axis=1)\n",
    "\n",
    "\n",
    "# final model\n",
    "scaler_final_model = StandardScaler()\n",
    "scaler_final_model = scaler_final_model.fit(train_data_final_model.loc[:, numerical_predictor_features])\n",
    "train_final_scaled_features = scaler_final_model.transform(train_data_final_model.loc[:, numerical_predictor_features])\n",
    "train_final_scaled_features = pd.DataFrame(train_final_scaled_features)\n",
    "train_final_scaled_features.columns = numerical_predictor_features\n",
    "train_data_final_model = train_data_final_model.drop(numerical_predictor_features, axis=1).reset_index(level=0, drop=True)\n",
    "train_data_final_model = pd.concat((train_data_final_model, train_final_scaled_features), axis=1)\n",
    "\n",
    "score_final_scaled_features = scaler_final_model.transform(score_data_final_model.loc[:, numerical_predictor_features])\n",
    "score_final_scaled_features = pd.DataFrame(score_final_scaled_features)\n",
    "score_final_scaled_features.columns = numerical_predictor_features\n",
    "score_data_final_model = score_data_final_model.drop(numerical_predictor_features, axis=1).reset_index(level=0, drop=True)\n",
    "score_data_final_model = pd.concat((score_data_final_model, score_final_scaled_features), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost\n",
    "\n",
    "\n",
    "# define predictor and target feature. So far only categorical features are in the model\n",
    "predictor_features = numerical_predictor_features\n",
    "target_feature = 'violation_in_following_120_days'\n",
    "\n",
    "# create parameter grid\n",
    "xgb_param_grid ={\n",
    "    'nthread': [1],  # use maximum number of threads\n",
    "    'objective': ['binary:logistic'],\n",
    "    \"grow_policy\": [\"lossguide\"],\n",
    "    \"num_class\": [1],\n",
    "    \"tree_method\": [\"hist\"],\n",
    "    \"max_depth\": [5,8,9,12,15], \n",
    "    \"min_child_weight\": [2,4,6,8],  \n",
    "    \"colsample_bytree\": [0.7,0.75,0.8,0.85],\n",
    "    \"gamma\": [0, 1],\n",
    "    \"subsample\": [0.7,0.8,0.9,0.95],\n",
    "    'learning_rate': [0.01, 0.07, 0.1],\n",
    "    'silent': [1],\n",
    "    'seed': [1337],\n",
    "    \"n_estimators\": [10, 15, 20,40,60]\n",
    "    }\n",
    "\n",
    "\n",
    "# choosing the first model\n",
    "best_score = 0\n",
    "best_params = None\n",
    "for param in ParameterGrid(xgb_param_grid):\n",
    "    # fit the model\n",
    "    model = xgboost.XGBClassifier(**param)\n",
    "    model.fit(train_data[predictor_features],train_data[target_feature])\n",
    "    \n",
    "    # get auc\n",
    "    predicted_values = model.predict_proba(validation_data[predictor_features])[:,1]\n",
    "    auc = roc_auc_score(validation_data['violation_in_following_120_days'], predicted_values)\n",
    "    \n",
    "\n",
    "    # save if best\n",
    "    if auc > best_score:\n",
    "        best_score = auc\n",
    "        best_params = model.get_xgb_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary:logistic', 'base_score': 0.5, 'booster': None, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.75, 'gamma': 1, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': None, 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 8, 'min_child_weight': 2, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 60, 'n_jobs': 1, 'num_parallel_tree': 1, 'random_state': 1337, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'subsample': 0.9, 'tree_method': 'hist', 'validate_parameters': False, 'verbosity': None, 'grow_policy': 'lossguide', 'nthread': 1, 'num_class': 1, 'seed': 1337, 'silent': 1}\n"
     ]
    }
   ],
   "source": [
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaulation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBClassifier(**best_params)\n",
    "model.fit(train_data_second_model[predictor_features],train_data_second_model[target_feature])\n",
    "predicted_values = model.predict_proba(test_data[predictor_features])[:,1]\n",
    "auc = roc_auc_score(test_data['violation_in_following_120_days'], predicted_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc 0.7822114937684761\n"
     ]
    }
   ],
   "source": [
    "print(\"test auc\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_data['violation_in_following_120_days'], predicted_values)\n",
    "accuracy_scores = []\n",
    "for thresh in thresholds:\n",
    "    accuracy_scores.append(accuracy_score(test_data['violation_in_following_120_days'], \n",
    "                                         [1 if m > thresh else 0 for m in predicted_values]))\n",
    "\n",
    "accuracies = np.array(accuracy_scores)\n",
    "max_accuracy = accuracies.max() \n",
    "max_accuracy_threshold =  thresholds[accuracies.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8173162090345438\n"
     ]
    }
   ],
   "source": [
    "print(max_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost\n",
    "\n",
    "\n",
    "# define predictor and target feature. So far only categorical features are in the model\n",
    "predictor_features = numerical_predictor_features\n",
    "target_feature = 'N_violations_next_insp'\n",
    "# create parameter grid\n",
    "xgb_param_grid ={\n",
    "    'nthread': [1],  # use maximum number of threads\n",
    "    'objective': ['reg:squarederror'],\n",
    "    \"grow_policy\": [\"lossguide\"],\n",
    "    # \"num_class\": [1],\n",
    "    \"tree_method\": [\"hist\"],\n",
    "    \"max_depth\": [4,5,6,7,8], \n",
    "    \"min_child_weight\": [2,4,6,8],  \n",
    "    \"colsample_bytree\": [0.7,0.75,0.8,0.85],\n",
    "    \"gamma\": [0, 1],\n",
    "    \"subsample\": [0.7,0.8,0.9,0.95],\n",
    "    'learning_rate': [0.01, 0.07, 0.1],\n",
    "    'silent': [1],\n",
    "    'seed': [1337],\n",
    "    \"n_estimators\": [10, 15, 20,40,60]\n",
    "    }\n",
    "\n",
    "\n",
    "# choosing the first model\n",
    "regressor_best_score = 1000\n",
    "regressor_best_params = None\n",
    "for param in ParameterGrid(xgb_param_grid):\n",
    "    # fit the model\n",
    "    model = xgboost.XGBRegressor(**param)\n",
    "    model.fit(train_data[predictor_features],train_data[target_feature])\n",
    "    \n",
    "    # get auc\n",
    "    predicted_values = model.predict(validation_data[predictor_features])\n",
    "    mae = mean_absolute_error(validation_data[target_feature], predicted_values)\n",
    "    \n",
    "\n",
    "    # save if best\n",
    "    if mae < regressor_best_score:\n",
    "        regressor_best_score = mae\n",
    "        regressor_best_params = model.get_xgb_params()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_model = xgboost.XGBRegressor(**regressor_best_params)\n",
    "regressor_model.fit(train_data_second_model[predictor_features],train_data_second_model[target_feature])\n",
    "regressor_predicted_values = regressor_model.predict(test_data[predictor_features])\n",
    "mae = mean_absolute_error(test_data[target_feature], regressor_predicted_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mae 2.5526421153689682\n"
     ]
    }
   ],
   "source": [
    "print(\"test mae\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6310156480661355\n"
     ]
    }
   ],
   "source": [
    "print(test_data[target_feature].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
